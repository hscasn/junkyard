{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze | grep apache-beam==2.10.0 || pip install apache-beam[gcp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze | grep tensorflow==1.12.0 || pip install tensorflow==1.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze | grep Pillow > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'qwiklabs-gcp-3f19cbba7aa3ae63'\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['CLOUDSDK_PYTHON'] = 'python3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "#standardSQL\n",
    "SELECT DISTINCT product_id, image_url FROM `qwiklabs-gcp-3f19cbba7aa3ae63.project.raw`\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take the image url and product id, and fetch the actual image\n",
    "class TransformImages(beam.DoFn):\n",
    "  def process(self, element):\n",
    "    import requests\n",
    "    from PIL import Image\n",
    "    import logging\n",
    "    import io\n",
    "    \n",
    "    uri = element['image_url']\n",
    "    id = element['product_id']\n",
    "    \n",
    "    # some urls are missing the \"http:\" part\n",
    "    if uri[:2] == '//':\n",
    "        uri = 'http:' + uri\n",
    "    \n",
    "    try:\n",
    "      res = requests.get(url=uri)\n",
    "      image_bytes = res.content\n",
    "      img = Image.open(io.BytesIO(image_bytes)).convert('RGB')\n",
    "    \n",
    "      # if you want to resize the image, uncomment the next line\n",
    "      #img = img.resize((250, 250), Image.ANTIALIAS)\n",
    "    except Exception as e:\n",
    "      logging.exception('Error processing image %s: %s', uri, str(e))\n",
    "      return\n",
    "\n",
    "    output = io.BytesIO()\n",
    "    img.save(output, 'jpeg')\n",
    "    image_bytes = output.getvalue()\n",
    "    yield id, image_bytes\n",
    "    \n",
    "# This will write the image bytes to GCS\n",
    "class WriteToStorage(beam.DoFn):\n",
    "  def process(self, element):\n",
    "    from apache_beam.io import filesystems\n",
    "    (name, image_bytes) = element\n",
    "    path = 'gs://project-sample/dataset1/{}.jpeg'.format(name)\n",
    "    writer = filesystems.FileSystems.create(path)\n",
    "    writer.write(image_bytes)\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "def preprocess(runner):\n",
    "  job_name = 'test-preprocess-images' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S')\n",
    "  print('Launching Dataflow job {} ... hang on'.format(job_name))\n",
    "\n",
    "  options = {\n",
    "    'staging_location': 'gs://project-sample/out/tmp/staging',\n",
    "    'temp_location': 'gs://project-sample/out/tmp',\n",
    "    'job_name': job_name,\n",
    "    'requirements_file': 'requirements.txt',\n",
    "    'project': PROJECT,\n",
    "    'runner': runner,\n",
    "  }\n",
    "  \n",
    "  #instantiate PipelineOptions object using options dictionary\n",
    "  opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "\n",
    "  #instantantiate Pipeline object using PipelineOptions\n",
    "  with beam.Pipeline(options=opts) as p:\n",
    "    (\n",
    "      p | 'read' >> beam.io.Read(beam.io.BigQuerySource(query=query, use_standard_sql=True))\n",
    "        | 'transform' >> beam.ParDo(TransformImages())\n",
    "        | 'write' >> beam.ParDo(WriteToStorage())\n",
    "    )\n",
    "  print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(\"DirectRunner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(\"DataflowRunner\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
