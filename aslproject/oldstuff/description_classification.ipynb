{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT = \"qwiklabs-gcp-3f19cbba7aa3ae63\" # REPLACE WITH YOUR PROJECT ID\n",
    "BUCKET = \"project-sample\" # REPLACE WITH YOUR BUCKET NAME\n",
    "REGION = \"us-central1\" # REPLACE WITH YOUR BUCKET REGION e.g. us-central1\n",
    "\n",
    "# do not change these\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"TFVERSION\"] = \"1.8\"  # Tensorflow version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting description_model/trainer/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile description_model/trainer/__init__.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached https://files.pythonhosted.org/packages/d2/ea/ab2c8c0e81bd051cc1180b104c75a865ab0fc66c89be992c4b20bbf6d624/tensorflow-1.13.1-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting keras-applications>=1.0.6 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/90/85/64c82949765cfb246bbdaf5aca2d55f400f792655927a017710a78445def/Keras_Applications-1.0.7-py2.py3-none-any.whl\n",
      "Collecting mock>=2.0.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/e6/35/f187bdf23be87092bd0f1200d43d23076cee4d0dec109f195173fd3ebc79/mock-2.0.0-py2.py3-none-any.whl\n",
      "Collecting grpcio>=1.8.6 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/b8/be/3bb6d8241b5ed1f8437169df53e7dd6ca986174e022585de15087a848c99/grpcio-1.19.0-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl\n",
      "Collecting numpy>=1.13.3 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/c4/33/8ec8dcdb4ede5d453047bbdbd01916dbaccdb63e98bba60989718f5f0876/numpy-1.16.2-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "Collecting backports.weakref>=1.0rc1 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/88/ec/f598b633c3d5ffe267aaada57d961c94fdfa183c5c3ebda2b6d151943db6/backports.weakref-1.0.post1-py2.py3-none-any.whl\n",
      "Collecting absl-py>=0.1.6 (from tensorflow)\n",
      "Collecting wheel (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/96/ba/a4702cbb6a3a485239fbe9525443446203f00771af9ac000fa3ef2788201/wheel-0.33.1-py2.py3-none-any.whl\n",
      "Collecting tensorboard<1.14.0,>=1.13.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/89/ac/48dd71c2bdc8d31e367f9b72f25ccb3b89bc6b9d664fee21f9a8efa5714d/tensorboard-1.13.1-py2-none-any.whl\n",
      "Collecting six>=1.10.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
      "Collecting gast>=0.2.0 (from tensorflow)\n",
      "Collecting keras-preprocessing>=1.0.5 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/c0/bf/0315ef6a9fd3fc2346e85b0ff1f5f83ca17073f2c31ac719ab2e4da0d4a3/Keras_Preprocessing-1.0.9-py2.py3-none-any.whl\n",
      "Collecting protobuf>=3.6.1 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/ea/72/5eadea03b06ca1320be2433ef2236155da17806b700efc92677ee99ae119/protobuf-3.7.1-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting enum34>=1.1.6 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/c5/db/e56e6b4bbac7c4a06de1c50de6fe1ef3810018ae11732a50f15f62c7d050/enum34-1.1.6-py2-none-any.whl\n",
      "Collecting astor>=0.6.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/35/6b/11530768cac581a12952a2aad00e1526b89d242d0b9f59534ef6e6a1752f/astor-0.7.1-py2.py3-none-any.whl\n",
      "Collecting h5py (from keras-applications>=1.0.6->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/53/08/27e4e9a369321862ffdce80ff1770553e9daec65d98befb2e14e7478b698/h5py-2.9.0-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting funcsigs>=1; python_version < \"3.3\" (from mock>=2.0.0->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
      "Collecting pbr>=0.11 (from mock>=2.0.0->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/14/09/12fe9a14237a6b7e0ba3a8d6fcf254bf4b10ec56a0185f73d651145e9222/pbr-5.1.3-py2.py3-none-any.whl\n",
      "Collecting futures>=2.2.0 (from grpcio>=1.8.6->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/2d/99/b2c4e9d5a30f6471e410a146232b4118e697fa3ffc06d6a65efde84debd0/futures-3.2.0-py2-none-any.whl\n",
      "Collecting werkzeug>=0.11.15 (from tensorboard<1.14.0,>=1.13.0->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/18/79/84f02539cc181cdbf5ff5a41b9f52cae870b6f632767e43ba6ac70132e92/Werkzeug-0.15.2-py2.py3-none-any.whl\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.14.0,>=1.13.0->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/f5/e4/d8c18f2555add57ff21bf25af36d827145896a07607486cc79a2aea641af/Markdown-3.1-py2.py3-none-any.whl\n",
      "Collecting setuptools (from protobuf>=3.6.1->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/d1/6a/4b2fcefd2ea0868810e92d519dacac1ddc64a2e53ba9e3422c3b62b378a6/setuptools-40.8.0-py2.py3-none-any.whl\n",
      "Installing collected packages: numpy, six, h5py, keras-applications, funcsigs, pbr, mock, futures, enum34, grpcio, absl-py, tensorflow-estimator, termcolor, backports.weakref, wheel, setuptools, protobuf, werkzeug, markdown, tensorboard, gast, keras-preprocessing, astor, tensorflow\n",
      "Successfully installed absl-py-0.7.1 astor-0.7.1 backports.weakref-1.0.post1 enum34-1.1.6 funcsigs-1.0.2 futures-3.2.0 gast-0.2.2 grpcio-1.19.0 h5py-2.9.0 keras-applications-1.0.7 keras-preprocessing-1.0.9 markdown-3.1 mock-2.0.0 numpy-1.16.2 pbr-5.1.3 protobuf-3.7.1 setuptools-40.8.0 six-1.12.0 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0 termcolor-1.1.0 werkzeug-0.15.2 wheel-0.33.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting description_model/setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile description_model/setup.py\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "REQUIRED_PACKAGES = ['requests==2.19.1'] #required for python GCS client\n",
    "\n",
    "setup(\n",
    "    name='description_classification',\n",
    "    version='1.0',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='model for text classification'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting description_model/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile description_model/trainer/task.py\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "from . import model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # parse command line arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--output_dir',\n",
    "        help='GCS location to write checkpoints and export models',\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--train_data_path',\n",
    "        help='can be a local path or a GCS url (gs://...)',\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--eval_data_path',\n",
    "        help='can be a local path or a GCS url (gs://...)',\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--embedding_path',\n",
    "        help='OPTIONAL: can be a local path or a GCS url (gs://...). \\\n",
    "              Download from: https://nlp.stanford.edu/projects/glove/',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--num_epochs',\n",
    "        help='number of times to go through the data, default=10',\n",
    "        default=10,\n",
    "        type=float\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--batch_size',\n",
    "        help='number of records to read during each training step, default=128',\n",
    "        default=128,\n",
    "        type=int\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--learning_rate',\n",
    "        help='learning rate for gradient descent, default=.001',\n",
    "        default=.001,\n",
    "        type=float\n",
    "    )\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "    hparams = args.__dict__\n",
    "    output_dir = hparams.pop('output_dir')\n",
    "    \n",
    "    model.train_and_evaluate(output_dir, hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting description_model/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile description_model/trainer/model.py\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "from tensorflow.python.keras.preprocessing import text\n",
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "from tensorflow.python.keras.layers import Embedding\n",
    "from tensorflow.python.keras.layers import Conv1D\n",
    "from tensorflow.python.keras.layers import MaxPooling1D\n",
    "from tensorflow.python.keras.layers import GlobalAveragePooling1D\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "CLASSES_LIST = ['Bras', 'Ties', 'Tops', 'Jeans', 'Polos', 'Rings', 'Socks',\n",
    "       'Skirts', 'Watches', 'Leggings', 'Sweaters', 'T-Shirts',\n",
    "       'Necklaces', 'Swim Tops', 'Underwear', 'Fragrances', 'Range Hoods',\n",
    "       'Basins/Sinks', 'Button-Downs', 'Slacks/Pants', 'Swim Bottoms',\n",
    "       'Jackets/Coats', 'Office Chairs', 'Gloves/Mittens',\n",
    "       'Semi-Brim Hats', 'Dresses & Gowns', 'Pendants & Charms',\n",
    "       'Blazers/Suit Coats', 'Swim Variety Packs', 'Bracelets & Anklets',\n",
    "       'One-Piece Swimsuits', 'Protective Footwear',\n",
    "       'Faucets/Taps/Handles', 'Bedding Variety Packs',\n",
    "       'Earrings & Ear Jewelry', 'Protective/Active Tops',\n",
    "       'Cardigans/Kimonos/Wraps', 'Everyday/Dress Footwear',\n",
    "       'Protective/Active Pants', 'Protective/Active Vests',\n",
    "       'Tableware Variety Packs', 'Active/Athletic Footwear',\n",
    "       'Protective/Active Shorts', 'Specialty Sport Footwear',\n",
    "       'Hair Cleaning & Treatments', 'Business/Formal Dress Suits',\n",
    "       'Sweatshirts/Fleece Pullovers', 'Clothing Sets & Variety Packs',\n",
    "       'Protective/Active Button-Downs',\n",
    "       'Vitamins, Minerals, & Dietary Supplements']\n",
    "CLASSES = { CLASSES_LIST[i]: i for i in range(len(CLASSES_LIST))}\n",
    "TOP_K = 20000  # Limit on the number vocabulary size used for tokenization\n",
    "MAX_SEQUENCE_LENGTH = 500  # Sentences will be truncated/padded to this length\n",
    "PADWORD = 'ZYXW'\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "  # Arguments:\n",
    "      train_data_path: string, path to tsv containing training data.\n",
    "        can be a local path or a GCS url (gs://...)\n",
    "      eval_data_path: string, path to tsv containing eval data.\n",
    "        can be a local path or a GCS url (gs://...)\n",
    "  # Returns:\n",
    "      ((train_sentences, train_labels), (test_sentences, test_labels)):  sentences\n",
    "        are lists of strings, labels are numpy integer arrays\n",
    "\"\"\"\n",
    "def load_data(train_data_path, eval_data_path, field):\n",
    "    column_names = ('bucket_name', 'product_id', 'product_name', 'description')\n",
    "\n",
    "    def download_from_gcs(source, destination):\n",
    "        search = re.search('gs://(.*?)/(.*)', source)\n",
    "        bucket_name = search.group(1)\n",
    "        blob_name = search.group(2)\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.get_bucket(bucket_name)\n",
    "        bucket.blob(blob_name).download_to_filename(destination)\n",
    "\n",
    "    if train_data_path.startswith('gs://'):\n",
    "        download_from_gcs(train_data_path, destination='train.csv')\n",
    "        train_data_path = 'train.csv'\n",
    "    if eval_data_path.startswith('gs://'):\n",
    "        download_from_gcs(eval_data_path, destination='eval.csv')\n",
    "        eval_data_path = 'eval.csv'\n",
    "\n",
    "    # Parse CSV using pandas\n",
    "    df_train = pd.read_csv(train_data_path)\n",
    "    df_eval = pd.read_csv(eval_data_path)\n",
    "\n",
    "    return ((list(df_train[field]), np.array(df_train['bucket_name'].map(CLASSES))),\n",
    "            (list(df_eval[field]), np.array(df_eval['bucket_name'].map(CLASSES))))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Create tf.estimator compatible input function\n",
    "  # Arguments:\n",
    "      texts: [strings], list of sentences\n",
    "      labels: numpy int vector, integer labels for sentences\n",
    "      batch_size: int, number of records to use for each train batch\n",
    "      mode: tf.estimator.ModeKeys.TRAIN or tf.estimator.ModeKeys.EVAL \n",
    "  # Returns:\n",
    "      tf.data.Dataset, produces feature and label\n",
    "        tensors one batch at a time\n",
    "\"\"\"\n",
    "def input_fn(texts, labels, batch_size, mode):\n",
    "    # Convert texts from python strings to tensors\n",
    "    x = tf.constant(texts)\n",
    "\n",
    "    # Map text to sequence of word-integers and pad\n",
    "    x = vectorize_sentences(x)\n",
    "\n",
    "    # Create tf.data.Dataset from tensors\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, labels))\n",
    "\n",
    "    # Pad to constant length\n",
    "    dataset = dataset.map(pad)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        num_epochs = None #loop indefinitley\n",
    "        dataset = dataset.shuffle(buffer_size=50000) # our input is already shuffled so this is redundant\n",
    "    else:\n",
    "        num_epochs = 1\n",
    "\n",
    "    dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "\"\"\"\n",
    "Given an int tensor, remove 0s then pad to a fixed length representation. \n",
    "  #Arguments:\n",
    "    feature: int tensor \n",
    "    label: int. not used in function, just passed through\n",
    "  #Returns:\n",
    "    (int tensor, int) tuple.\n",
    "\"\"\"\n",
    "def pad(feature, label):\n",
    "    # 1. Remove 0s which represent out of vocabulary words\n",
    "    nonzero_indices = tf.where(tf.not_equal(feature, tf.zeros_like(feature)))\n",
    "    without_zeros = tf.gather(feature,nonzero_indices)\n",
    "    without_zeros = tf.squeeze(without_zeros, axis=1)\n",
    "\n",
    "    # 2. Prepend 0s till MAX_SEQUENCE_LENGTH\n",
    "    padded = tf.pad(without_zeros, [[MAX_SEQUENCE_LENGTH, 0]])  # pad out with zeros\n",
    "    padded = padded[-MAX_SEQUENCE_LENGTH:]  # slice to constant length\n",
    "    return (padded, label)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Given sentences, return an integer representation\n",
    "  # Arguments:\n",
    "      sentences: string tensor of shape (?,), contains sentences to vectorize\n",
    "  # Returns:\n",
    "      Integer representation of the sentence. Word-integer mapping is determined\n",
    "        by VOCAB_FILE_PATH. Words out of vocabulary will map to 0\n",
    "\"\"\"\n",
    "def vectorize_sentences(sentences):\n",
    "    # 1. Remove punctuation\n",
    "    sentences = tf.regex_replace(sentences, '[[:punct:]]', ' ')\n",
    "\n",
    "    # 2. Split string tensor into component words\n",
    "    words = tf.string_split(sentences)\n",
    "    words = tf.sparse_tensor_to_dense(words, default_value=PADWORD)\n",
    "\n",
    "    # 3. Map each word to respective integer\n",
    "    table = tf.contrib.lookup.index_table_from_file(\n",
    "        vocabulary_file=VOCAB_FILE_PATH,\n",
    "        num_oov_buckets=0,\n",
    "        vocab_size=None,\n",
    "        default_value=0,  # for words not in vocabulary (OOV)\n",
    "        key_column_index=0,\n",
    "        value_column_index=1,\n",
    "        delimiter=',')\n",
    "    numbers = table.lookup(words)\n",
    "\n",
    "    return numbers\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Builds a CNN model using keras and converts to tf.estimator.Estimator\n",
    "  # Arguments\n",
    "      model_dir: string, file path where training files will be written\n",
    "      config: tf.estimator.RunConfig, specifies properties of tf Estimator\n",
    "      filters: int, output dimension of the layers.\n",
    "      kernel_size: int, length of the convolution window.\n",
    "      embedding_dim: int, dimension of the embedding vectors.\n",
    "      dropout_rate: float, percentage of input to drop at Dropout layers.\n",
    "      pool_size: int, factor by which to downscale input at MaxPooling layer.\n",
    "      embedding_path: string , file location of pre-trained embedding (if used)\n",
    "        defaults to None which will cause the model to train embedding from scratch\n",
    "      word_index: dictionary, mapping of vocabulary to integers. used only if\n",
    "        pre-trained embedding is provided\n",
    "\n",
    "    # Returns\n",
    "        A tf.estimator.Estimator \n",
    "\"\"\"\n",
    "def keras_estimator(model_dir,\n",
    "                    config,\n",
    "                    learning_rate,\n",
    "                    filters=64,\n",
    "                    dropout_rate=0.2,\n",
    "                    embedding_dim=200,\n",
    "                    kernel_size=3,\n",
    "                    pool_size=3,\n",
    "                    embedding_path=None,\n",
    "                    word_index=None):\n",
    "    # Create model instance.\n",
    "    model = models.Sequential()\n",
    "    num_features = min(len(word_index) + 1, TOP_K)\n",
    "\n",
    "    # Add embedding layer. If pre-trained embedding is used add weights to the\n",
    "    # embeddings layer and set trainable to input is_embedding_trainable flag.\n",
    "    if embedding_path != None:\n",
    "        embedding_matrix = get_embedding_matrix(word_index, embedding_path, embedding_dim)\n",
    "        is_embedding_trainable = True  # set to False to freeze embedding weights\n",
    "\n",
    "        model.add(Embedding(input_dim=num_features,\n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=is_embedding_trainable))\n",
    "    else:\n",
    "        model.add(Embedding(input_dim=num_features,\n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "    model.add(Conv1D(filters=filters,\n",
    "                              kernel_size=kernel_size,\n",
    "                              activation='relu',\n",
    "                              bias_initializer='random_uniform',\n",
    "                              padding='same'))\n",
    "\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(Conv1D(filters=filters * 2,\n",
    "                              kernel_size=kernel_size,\n",
    "                              activation='relu',\n",
    "                              bias_initializer='random_uniform',\n",
    "                              padding='same'))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "    model.add(Dense(len(CLASSES), activation='softmax'))\n",
    "\n",
    "    # Compile model with learning parameters.\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "    estimator = tf.keras.estimator.model_to_estimator(keras_model=model, model_dir=model_dir, config=config)\n",
    "\n",
    "    return estimator\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Defines the features to be passed to the model during inference\n",
    "  Can pass in string text directly. Tokenization done in serving_input_fn \n",
    "  # Arguments: none\n",
    "  # Returns: tf.estimator.export.ServingInputReceiver\n",
    "\"\"\"\n",
    "def serving_input_fn():\n",
    "    feature_placeholder = tf.placeholder(tf.string, [None])\n",
    "    features = vectorize_sentences(feature_placeholder)\n",
    "    return tf.estimator.export.TensorServingInputReceiver(features, feature_placeholder)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Takes embedding for generic vocabulary and extracts the embeddings\n",
    "  matching the current vocabulary\n",
    "  The pre-trained embedding file is obtained from https://nlp.stanford.edu/projects/glove/\n",
    "  # Arguments: \n",
    "      word_index: dict, {key =word in vocabulary: value= integer mapped to that word}\n",
    "      embedding_path: string, location of the pre-trained embedding file on disk\n",
    "      embedding_dim: int, dimension of the embedding space\n",
    "  # Returns: numpy matrix of shape (vocabulary, embedding_dim) that contains the embedded\n",
    "      representation of each word in the vocabulary.\n",
    "\"\"\"\n",
    "def get_embedding_matrix(word_index, embedding_path, embedding_dim):\n",
    "    # Read the pre-trained embedding file and get word to word vector mappings.\n",
    "    embedding_matrix_all = {}\n",
    "\n",
    "    # Download if embedding file is in GCS\n",
    "    if embedding_path.startswith('gs://'):\n",
    "        download_from_gcs(embedding_path, destination='embedding.csv')\n",
    "        embedding_path = 'embedding.csv'\n",
    "\n",
    "    with open(embedding_path) as f:\n",
    "        for line in f:  # Every line contains word followed by the vector value\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embedding_matrix_all[word] = coefs\n",
    "\n",
    "    # Prepare embedding matrix with just the words in our word_index dictionary\n",
    "    num_words = min(len(word_index) + 1, TOP_K)\n",
    "    embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        if i >= TOP_K:\n",
    "            continue\n",
    "        embedding_vector = embedding_matrix_all.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Main orchestrator. Responsible for calling all other functions in model.py\n",
    "  # Arguments: \n",
    "      output_dir: string, file path where training files will be written\n",
    "      hparams: dict, command line parameters passed from task.py\n",
    "  # Returns: nothing, kicks off training and evaluation\n",
    "\"\"\"\n",
    "def train_and_evaluate(output_dir, hparams):\n",
    "    # Load Data\n",
    "    ((train_texts, train_labels), (test_texts, test_labels)) = load_data(\n",
    "        hparams['train_data_path'], hparams['eval_data_path'], 'description')\n",
    "\n",
    "    # Create vocabulary from training corpus.\n",
    "    tokenizer = text.Tokenizer(num_words=TOP_K)\n",
    "    tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "    # Generate vocabulary file from tokenizer object to enable\n",
    "    # creating a native tensorflow lookup table later (used in vectorize_sentences())\n",
    "    tf.gfile.MkDir(output_dir) # directory must exist before we can use tf.gfile.open\n",
    "    global VOCAB_FILE_PATH; VOCAB_FILE_PATH = os.path.join(output_dir,'vocab.txt')\n",
    "    with tf.gfile.Open(VOCAB_FILE_PATH, 'wb') as f:\n",
    "        f.write(\"{},0\\n\".format(PADWORD))  # map padword to 0\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index < TOP_K: # only save mappings for TOP_K words\n",
    "                f.write(\"{},{}\\n\".format(word, index))\n",
    "\n",
    "    # Create estimator\n",
    "    run_config = tf.estimator.RunConfig(save_checkpoints_steps=500)\n",
    "    estimator = keras_estimator(\n",
    "        model_dir=output_dir,\n",
    "        config=run_config,\n",
    "        learning_rate=hparams['learning_rate'],\n",
    "        embedding_path=hparams['embedding_path'],\n",
    "        word_index=tokenizer.word_index\n",
    "    )\n",
    "\n",
    "    # Create TrainSpec\n",
    "    train_steps = hparams['num_epochs'] * len(train_texts) / hparams['batch_size']\n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn=lambda:input_fn(\n",
    "            train_texts,\n",
    "            train_labels,\n",
    "            hparams['batch_size'],\n",
    "            mode=tf.estimator.ModeKeys.TRAIN),\n",
    "        max_steps=train_steps\n",
    "    )\n",
    "\n",
    "    # Create EvalSpec\n",
    "    exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn=lambda:input_fn(\n",
    "            test_texts,\n",
    "            test_labels,\n",
    "            hparams['batch_size'],\n",
    "            mode=tf.estimator.ModeKeys.EVAL),\n",
    "        steps=None,\n",
    "        exporters=exporter,\n",
    "        start_delay_secs=10,\n",
    "        throttle_secs=10\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-storage\n",
      "  Using cached https://files.pythonhosted.org/packages/b6/e9/06d9bb394fddbc62bb9c645f5e1c927128930a249d0c6a7491c3f31a9ff4/google_cloud_storage-1.14.0-py2.py3-none-any.whl\n",
      "Collecting google-cloud-core<0.30dev,>=0.29.0 (from google-cloud-storage)\n",
      "  Using cached https://files.pythonhosted.org/packages/0c/f2/3c225e7a69cb27d283b68bff867722bd066bc1858611180197f711815ea5/google_cloud_core-0.29.1-py2.py3-none-any.whl\n",
      "Collecting google-api-core<2.0.0dev,>=1.6.0 (from google-cloud-storage)\n",
      "  Using cached https://files.pythonhosted.org/packages/7d/73/e4877e921fe59307ec6b1b0b0c2ad9fde2d1c6bab8dd06ec913891a20dc6/google_api_core-1.8.2-py2.py3-none-any.whl\n",
      "Collecting google-resumable-media>=0.3.1 (from google-cloud-storage)\n",
      "  Using cached https://files.pythonhosted.org/packages/e2/5d/4bc5c28c252a62efe69ed1a1561da92bd5af8eca0cdcdf8e60354fae9b29/google_resumable_media-0.3.2-py2.py3-none-any.whl\n",
      "Collecting requests<3.0.0dev,>=2.18.0 (from google-api-core<2.0.0dev,>=1.6.0->google-cloud-storage)\n",
      "  Using cached https://files.pythonhosted.org/packages/7d/e3/20f3d364d6c8e5d2353c72a67778eb189176f08e873c9900e10c0287b84b/requests-2.21.0-py2.py3-none-any.whl\n",
      "Collecting pytz (from google-api-core<2.0.0dev,>=1.6.0->google-cloud-storage)\n",
      "  Using cached https://files.pythonhosted.org/packages/61/28/1d3920e4d1d50b19bc5d24398a7cd85cc7b9a75a490570d5a30c57622d34/pytz-2018.9-py2.py3-none-any.whl\n",
      "Collecting futures>=3.2.0; python_version < \"3.2\" (from google-api-core<2.0.0dev,>=1.6.0->google-cloud-storage)\n",
      "  Using cached https://files.pythonhosted.org/packages/2d/99/b2c4e9d5a30f6471e410a146232b4118e697fa3ffc06d6a65efde84debd0/futures-3.2.0-py2-none-any.whl\n",
      "Collecting six>=1.10.0 (from google-api-core<2.0.0dev,>=1.6.0->google-cloud-storage)\n",
      "  Using cached https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
      "Collecting setuptools>=34.0.0 (from google-api-core<2.0.0dev,>=1.6.0->google-cloud-storage)\n",
      "  Using cached https://files.pythonhosted.org/packages/d1/6a/4b2fcefd2ea0868810e92d519dacac1ddc64a2e53ba9e3422c3b62b378a6/setuptools-40.8.0-py2.py3-none-any.whl\n",
      "Collecting google-auth<2.0dev,>=0.4.0 (from google-api-core<2.0.0dev,>=1.6.0->google-cloud-storage)\n",
      "  Using cached https://files.pythonhosted.org/packages/c5/9b/ed0516cc1f7609fb0217e3057ff4f0f9f3e3ce79a369c6af4a6c5ca25664/google_auth-1.6.3-py2.py3-none-any.whl\n",
      "Collecting protobuf>=3.4.0 (from google-api-core<2.0.0dev,>=1.6.0->google-cloud-storage)\n",
      "  Using cached https://files.pythonhosted.org/packages/ea/72/5eadea03b06ca1320be2433ef2236155da17806b700efc92677ee99ae119/protobuf-3.7.1-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting googleapis-common-protos!=1.5.4,<2.0dev,>=1.5.3 (from google-api-core<2.0.0dev,>=1.6.0->google-cloud-storage)\n",
      "Collecting urllib3<1.25,>=1.21.1 (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.6.0->google-cloud-storage)\n",
      "  Using cached https://files.pythonhosted.org/packages/62/00/ee1d7de624db8ba7090d1226aebefab96a2c71cd5cfa7629d6ad3f61b79e/urllib3-1.24.1-py2.py3-none-any.whl\n",
      "Collecting idna<2.9,>=2.5 (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.6.0->google-cloud-storage)\n",
      "  Using cached https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl\n",
      "Collecting chardet<3.1.0,>=3.0.2 (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.6.0->google-cloud-storage)\n",
      "  Using cached https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl\n",
      "Collecting certifi>=2017.4.17 (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.6.0->google-cloud-storage)\n",
      "  Using cached https://files.pythonhosted.org/packages/60/75/f692a584e85b7eaba0e03827b3d51f45f571c2e793dd731e598828d380aa/certifi-2019.3.9-py2.py3-none-any.whl\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0dev,>=1.6.0->google-cloud-storage)\n",
      "  Using cached https://files.pythonhosted.org/packages/da/98/8ddd9fa4d84065926832bcf2255a2b69f1d03330aa4d1c49cc7317ac888e/pyasn1_modules-0.2.4-py2.py3-none-any.whl\n",
      "Collecting rsa>=3.1.4 (from google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0dev,>=1.6.0->google-cloud-storage)\n",
      "  Using cached https://files.pythonhosted.org/packages/02/e5/38518af393f7c214357079ce67a317307936896e961e35450b70fad2a9cf/rsa-4.0-py2.py3-none-any.whl\n",
      "Collecting cachetools>=2.0.0 (from google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0dev,>=1.6.0->google-cloud-storage)\n",
      "  Using cached https://files.pythonhosted.org/packages/39/2b/d87fc2369242bd743883232c463f28205902b8579cb68dcf5b11eee1652f/cachetools-3.1.0-py2.py3-none-any.whl\n",
      "Collecting pyasn1<0.5.0,>=0.4.1 (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0dev,>=1.6.0->google-cloud-storage)\n",
      "  Using cached https://files.pythonhosted.org/packages/7b/7c/c9386b82a25115cccf1903441bba3cbadcfae7b678a20167347fa8ded34c/pyasn1-0.4.5-py2.py3-none-any.whl\n",
      "Installing collected packages: urllib3, idna, chardet, certifi, requests, pytz, futures, six, setuptools, pyasn1, pyasn1-modules, rsa, cachetools, google-auth, protobuf, googleapis-common-protos, google-api-core, google-cloud-core, google-resumable-media, google-cloud-storage\n",
      "Successfully installed cachetools-3.1.0 certifi-2019.3.9 chardet-3.0.4 futures-3.2.0 google-api-core-1.8.2 google-auth-1.6.3 google-cloud-core-0.29.1 google-cloud-storage-1.14.0 google-resumable-media-0.3.2 googleapis-common-protos-1.5.9 idna-2.8 protobuf-3.7.1 pyasn1-0.4.5 pyasn1-modules-0.2.4 pytz-2018.9 requests-2.21.0 rsa-4.0 setuptools-40.8.0 six-1.12.0 urllib3-1.24.1\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:TF_CONFIG environment variable: {u'environment': u'cloud', u'cluster': {}, u'job': {u'args': [u'--output_dir=/home/jupyter/Final_Project/description_model_trained', u'--train_data_path=gs://project-sample/dataset1_data_train.csv', u'--eval_data_path=gs://project-sample/dataset1_data_eval.csv', u'--num_epochs=0.1'], u'job_name': u'trainer.task'}, u'task': {}}\n",
      "WARNING:tensorflow:From /home/jupyter/.local/lib/python2.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/jupyter/.local/lib/python2.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "INFO:tensorflow:Using the Keras model provided.\n",
      "2019-04-03 16:30:08.041639: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-04-03 16:30:08.047358: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
      "2019-04-03 16:30:08.048051: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x555b26ab64a0 executing computations on platform Host. Devices:\n",
      "2019-04-03 16:30:08.048088: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "WARNING:tensorflow:From /home/jupyter/.local/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd6bfd3d690>, '_model_dir': '/home/jupyter/Final_Project/description_model_trained', '_protocol': None, '_save_checkpoints_steps': 500, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_master': ''}\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 500 or save_checkpoints_secs None.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n",
      "    \"__main__\", fname, loader, pkg_name)\n",
      "  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n",
      "    exec code in run_globals\n",
      "  File \"/home/jupyter/Final_Project/description_model/trainer/task.py\", line 54, in <module>\n",
      "    model.train_and_evaluate(output_dir, hparams)\n",
      "  File \"trainer/model.py\", line 356, in train_and_evaluate\n",
      "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
      "  File \"/home/jupyter/.local/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/training.py\", line 471, in train_and_evaluate\n",
      "    return executor.run()\n",
      "  File \"/home/jupyter/.local/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/training.py\", line 611, in run\n",
      "    return self.run_local()\n",
      "  File \"/home/jupyter/.local/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/training.py\", line 712, in run_local\n",
      "    saving_listeners=saving_listeners)\n",
      "  File \"/home/jupyter/.local/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 358, in train\n",
      "    loss = self._train_model(input_fn, hooks, saving_listeners)\n",
      "  File \"/home/jupyter/.local/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1124, in _train_model\n",
      "    return self._train_model_default(input_fn, hooks, saving_listeners)\n",
      "  File \"/home/jupyter/.local/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1154, in _train_model_default\n",
      "    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\n",
      "  File \"/home/jupyter/.local/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1112, in _call_model_fn\n",
      "    model_fn_results = self._model_fn(features=features, **kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/keras.py\", line 278, in model_fn\n",
      "    labels)\n",
      "  File \"/home/jupyter/.local/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/keras.py\", line 201, in _clone_and_build_model\n",
      "    optimizer_iterations=global_step)\n",
      "  File \"/home/jupyter/.local/lib/python2.7/site-packages/tensorflow/python/keras/models.py\", line 511, in clone_and_build_model\n",
      "    target_tensors=target_tensors)\n",
      "  File \"/home/jupyter/.local/lib/python2.7/site-packages/tensorflow/python/training/checkpointable/base.py\", line 442, in _method_wrapper\n",
      "    method(self, *args, **kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python2.7/site-packages/tensorflow/python/keras/engine/training.py\", line 499, in compile\n",
      "    sample_weights=self.sample_weights)\n",
      "  File \"/home/jupyter/.local/lib/python2.7/site-packages/tensorflow/python/keras/engine/training.py\", line 1844, in _handle_metrics\n",
      "    return_stateful_result=return_stateful_result))\n",
      "  File \"/home/jupyter/.local/lib/python2.7/site-packages/tensorflow/python/keras/engine/training.py\", line 1800, in _handle_per_output_metrics\n",
      "    stateful_metric_result = _call_stateful_fn(stateful_fn)\n",
      "  File \"/home/jupyter/.local/lib/python2.7/site-packages/tensorflow/python/keras/engine/training.py\", line 1773, in _call_stateful_fn\n",
      "    fn, y_true, y_pred, weights=weights, mask=mask)\n",
      "  File \"/home/jupyter/.local/lib/python2.7/site-packages/tensorflow/python/keras/engine/training_utils.py\", line 852, in call_metric_function\n",
      "    return metric_fn(y_true, y_pred, sample_weight=weights)\n",
      "  File \"/home/jupyter/.local/lib/python2.7/site-packages/tensorflow/python/keras/metrics.py\", line 438, in __call__\n",
      "    update_op = self.update_state(*args, **kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python2.7/site-packages/tensorflow/python/keras/metrics.py\", line 160, in inner\n",
      "    return func.__get__(instance_ref(), cls)(*args, **kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python2.7/site-packages/tensorflow/python/keras/metrics.py\", line 98, in decorated\n",
      "    update_op = update_state_fn(*args, **kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python2.7/site-packages/tensorflow/python/keras/metrics.py\", line 647, in update_state\n",
      "    y_pred, y_true, sample_weight)\n",
      "  File \"/home/jupyter/.local/lib/python2.7/site-packages/tensorflow/python/keras/utils/losses_utils.py\", line 57, in squeeze_or_expand_dimensions\n",
      "    y_true, y_pred)\n",
      "  File \"/home/jupyter/.local/lib/python2.7/site-packages/tensorflow/python/ops/confusion_matrix.py\", line 71, in remove_squeezable_dimensions\n",
      "    predictions = array_ops.squeeze(predictions, [-1])\n",
      "  File \"/home/jupyter/.local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 3146, in squeeze\n",
      "    return gen_array_ops.squeeze(input, axis, name)\n",
      "  File \"/home/jupyter/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 9043, in squeeze\n",
      "    \"Squeeze\", input=input, squeeze_dims=axis, name=name)\n",
      "  File \"/home/jupyter/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"/home/jupyter/.local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/jupyter/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n",
      "    op_def=op_def)\n",
      "  File \"/home/jupyter/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1823, in __init__\n",
      "    control_input_ops)\n",
      "  File \"/home/jupyter/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1662, in _create_c_op\n",
      "    raise ValueError(str(e))\n",
      "ValueError: Can not squeeze dim[1], expected a dimension of 1, got 50 for 'metrics/acc/remove_squeezable_dimensions/Squeeze' (op: 'Squeeze') with input shapes: [?,50].\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'pip install --upgrade google-cloud-storage\\nrm -rf description_model_trained\\ngcloud ml-engine local train \\\\\\n   --module-name=trainer.task \\\\\\n   --package-path=${PWD}/description_model/trainer \\\\\\n   -- \\\\\\n   --output_dir=${PWD}/description_model_trained \\\\\\n   --train_data_path=gs://${BUCKET}/dataset1_data_train.csv \\\\\\n   --eval_data_path=gs://${BUCKET}/dataset1_data_eval.csv \\\\\\n   --num_epochs=0.1\\n'' returned non-zero exit status 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-f793dc6f60ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pip install --upgrade google-cloud-storage\\nrm -rf description_model_trained\\ngcloud ml-engine local train \\\\\\n   --module-name=trainer.task \\\\\\n   --package-path=${PWD}/description_model/trainer \\\\\\n   -- \\\\\\n   --output_dir=${PWD}/description_model_trained \\\\\\n   --train_data_path=gs://${BUCKET}/dataset1_data_train.csv \\\\\\n   --eval_data_path=gs://${BUCKET}/dataset1_data_eval.csv \\\\\\n   --num_epochs=0.1\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2345\u001b[0m                 \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2346\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2347\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2348\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m</usr/local/lib/python3.5/dist-packages/decorator.py:decorator-gen-110>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'pip install --upgrade google-cloud-storage\\nrm -rf description_model_trained\\ngcloud ml-engine local train \\\\\\n   --module-name=trainer.task \\\\\\n   --package-path=${PWD}/description_model/trainer \\\\\\n   -- \\\\\\n   --output_dir=${PWD}/description_model_trained \\\\\\n   --train_data_path=gs://${BUCKET}/dataset1_data_train.csv \\\\\\n   --eval_data_path=gs://${BUCKET}/dataset1_data_eval.csv \\\\\\n   --num_epochs=0.1\\n'' returned non-zero exit status 1"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install --upgrade google-cloud-storage\n",
    "rm -rf description_model_trained > /dev/null\n",
    "gcloud ml-engine local train \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=${PWD}/description_model/trainer \\\n",
    "   -- \\\n",
    "   --output_dir=${PWD}/description_model_trained \\\n",
    "   --train_data_path=gs://${BUCKET}/dataset1_data_train.csv \\\n",
    "   --eval_data_path=gs://${BUCKET}/dataset1_data_eval.csv \\\n",
    "   --num_epochs=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobId: description_classification_190405_135358\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing gs://project-sample/description_model/trained/#1554472284868789...\n",
      "Removing gs://project-sample/description_model/trained/export/#1554472121927654...\n",
      "Removing gs://project-sample/description_model/trained/eval/#1554472119960859...\n",
      "Removing gs://project-sample/description_model/trained/checkpoint#1554472286036567...\n",
      "Removing gs://project-sample/description_model/trained/eval/events.out.tfevents.1554472120.cmle-training-9817732626231145641#1554472289645796...\n",
      "Removing gs://project-sample/description_model/trained/events.out.tfevents.1554472097.cmle-training-9817732626231145641#1554472295459087...\n",
      "Removing gs://project-sample/description_model/trained/export/exporter/#1554472122133463...\n",
      "Removing gs://project-sample/description_model/trained/export/exporter/1554472121/#1554472126180212...\n",
      "Removing gs://project-sample/description_model/trained/export/exporter/1554472121/assets/#1554472126313585...\n",
      "Removing gs://project-sample/description_model/trained/export/exporter/1554472121/assets/vocab.txt#1554472126460946...\n",
      "Removing gs://project-sample/description_model/trained/export/exporter/1554472121/saved_model.pb#1554472126614976...\n",
      "Removing gs://project-sample/description_model/trained/export/exporter/1554472121/variables/#1554472126792321...\n",
      "Removing gs://project-sample/description_model/trained/export/exporter/1554472121/variables/variables.data-00000-of-00001#1554472126952607...\n",
      "Removing gs://project-sample/description_model/trained/export/exporter/1554472121/variables/variables.index#1554472127079798...\n",
      "Removing gs://project-sample/description_model/trained/export/exporter/1554472157/assets/#1554472162794478...\n",
      "Removing gs://project-sample/description_model/trained/export/exporter/1554472157/saved_model.pb#1554472163106140...\n",
      "Removing gs://project-sample/description_model/trained/export/exporter/1554472157/assets/vocab.txt#1554472162966304...\n",
      "Removing gs://project-sample/description_model/trained/export/exporter/1554472157/variables/#1554472163233205...\n",
      "Removing gs://project-sample/description_model/trained/export/exporter/1554472157/variables/variables.data-00000-of-00001#1554472163364946...\n",
      "Removing gs://project-sample/description_model/trained/export/exporter/1554472157/#1554472162656222...\n",
      "Removing gs://project-sample/description_model/trained/export/exporter/1554472157/variables/variables.index#1554472163490179...\n",
      "Removing gs://project-sample/description_model/trained/export/exporter/1554472193/#1554472197962360...\n",
      "Removing gs://project-sample/description_model/trained/export/exporter/1554472193/assets/#1554472198123539...\n",
      "Removing gs://project-sample/description_model/trained/export/exporter/1554472193/assets/vocab.txt#1554472198252101...\n",
      "Removing gs://project-sample/description_model/trained/export/exporter/1554472193/saved_model.pb#1554472198409755...\n",
      "Removing gs://project-sample/description_model/trained/export/exporter/1554472193/variables/#1554472198551792...\n",
      "Removing gs://project-sample/description_model/trained/export/exporter/1554472193/variables/variables.data-00000-of-00001#1554472198688972...\n",
      "Removing gs://project-sample/description_model/trained/export/exporter/1554472193/variables/variables.index#1554472198848369...\n",
      "Removing gs://project-sample/description_model/trained/export/exporter/1554472229/#1554472233717651...\n",
      "Removing gs://project-sample/description_model/trained/export/exporter/1554472229/assets/#1554472233881455...\n",
      "Removing gs://project-sample/description_model/trained/export/exporter/1554472229/assets/vocab.txt#1554472234030667...\n",
      "Removing gs://project-sample/description_model/trained/export/exporter/1554472229/saved_model.pb#1554472234211437...\n",
      "Removing gs://project-sample/description_model/trained/export/exporter/1554472229/variables/variables.data-00000-of-00001#1554472234510556...\n",
      "Removing gs://project-sample/description_model/trained/export/exporter/1554472229/variables/#1554472234380159...\n",
      "Removing gs://project-sample/description_model/trained/export/exporter/1554472229/variables/variables.index#1554472234675795...\n",
      "Removing gs://project-sample/description_model/trained/export/exporter/1554472265/#1554472269636996...\n",
      "Removing gs://project-sample/description_model/trained/export/exporter/1554472265/assets/#1554472269777271...\n",
      "Removing gs://project-sample/description_model/trained/export/exporter/1554472265/assets/vocab.txt#1554472269891538...\n",
      "Removing gs://project-sample/description_model/trained/export/exporter/1554472265/saved_model.pb#1554472270015522...\n",
      "Removing gs://project-sample/description_model/trained/export/exporter/1554472265/variables/#1554472270195634...\n",
      "Removing gs://project-sample/description_model/trained/export/exporter/1554472265/variables/variables.data-00000-of-00001#1554472270340814...\n",
      "Removing gs://project-sample/description_model/trained/export/exporter/1554472265/variables/variables.index#1554472270529051...\n",
      "Removing gs://project-sample/description_model/trained/export/exporter/1554472289/#1554472294288059...\n",
      "Removing gs://project-sample/description_model/trained/export/exporter/1554472289/assets/#1554472294436839...\n",
      "Removing gs://project-sample/description_model/trained/export/exporter/1554472289/assets/vocab.txt#1554472294558888...\n",
      "Removing gs://project-sample/description_model/trained/export/exporter/1554472289/saved_model.pb#1554472294680617...\n",
      "Removing gs://project-sample/description_model/trained/export/exporter/1554472289/variables/#1554472294806776...\n",
      "Removing gs://project-sample/description_model/trained/export/exporter/1554472289/variables/variables.data-00000-of-00001#1554472294940800...\n",
      "Removing gs://project-sample/description_model/trained/export/exporter/1554472289/variables/variables.index#1554472295080412...\n",
      "Removing gs://project-sample/description_model/trained/graph.pbtxt#1554472100533876...\n",
      "Removing gs://project-sample/description_model/trained/model.ckpt-1001.data-00000-of-00001#1554472188736723...\n",
      "Removing gs://project-sample/description_model/trained/model.ckpt-1001.index#1554472188948216...\n",
      "Removing gs://project-sample/description_model/trained/model.ckpt-1001.meta#1554472190143079...\n",
      "Removing gs://project-sample/description_model/trained/model.ckpt-1501.data-00000-of-00001#1554472224378019...\n",
      "Removing gs://project-sample/description_model/trained/model.ckpt-1501.index#1554472224652302...\n",
      "Removing gs://project-sample/description_model/trained/model.ckpt-1501.meta#1554472225788717...\n",
      "Removing gs://project-sample/description_model/trained/model.ckpt-2001.data-00000-of-00001#1554472260180761...\n",
      "Removing gs://project-sample/description_model/trained/model.ckpt-2001.index#1554472260388182...\n",
      "Removing gs://project-sample/description_model/trained/model.ckpt-2001.meta#1554472261937210...\n",
      "Removing gs://project-sample/description_model/trained/model.ckpt-2225.index#1554472285459305...\n",
      "Removing gs://project-sample/description_model/trained/model.ckpt-2225.data-00000-of-00001#1554472285232513...\n",
      "Removing gs://project-sample/description_model/trained/model.ckpt-2225.meta#1554472286923484...\n",
      "Removing gs://project-sample/description_model/trained/model.ckpt-501.data-00000-of-00001#1554472153123938...\n",
      "Removing gs://project-sample/description_model/trained/model.ckpt-501.index#1554472153374425...\n",
      "Removing gs://project-sample/description_model/trained/model.ckpt-501.meta#1554472154628387...\n",
      "Removing gs://project-sample/description_model/trained/packages/5f1af3ccf2c0206a57502b4135ccb86bce6a69beeed0de48ff95cdf384236f76/description_classification-1.0.tar.gz#1554471931485093...\n",
      "Removing gs://project-sample/description_model/trained/vocab.txt#1554472083604128...\n",
      "/ [67/67 objects] 100% Done                                                     \n",
      "Operation completed over 67 objects.                                             \n",
      "Job [description_classification_190405_135358] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs describe description_classification_190405_135358\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs stream-logs description_classification_190405_135358\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/description_model/trained\n",
    "JOBNAME=description_classification_$(date -u +%y%m%d_%H%M%S)\n",
    "REGION=us-central1\n",
    "gsutil -m rm -rf $OUTDIR > /dev/null\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    " --region=$REGION \\\n",
    " --module-name=trainer.task \\\n",
    " --package-path=${PWD}/description_model/trainer \\\n",
    " --job-dir=$OUTDIR \\\n",
    " --scale-tier=BASIC_GPU \\\n",
    " --runtime-version=$TFVERSION \\\n",
    " -- \\\n",
    " --output_dir=$OUTDIR \\\n",
    " --train_data_path=gs://${BUCKET}/dataset1_data_train.csv \\\n",
    " --eval_data_path=gs://${BUCKET}/dataset1_data_eval.csv \\\n",
    " --num_epochs=10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
