{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT = \"qwiklabs-gcp-3f19cbba7aa3ae63 \" # REPLACE WITH YOUR PROJECT ID\n",
    "BUCKET = \"project-sample\" # REPLACE WITH YOUR BUCKET NAME\n",
    "REGION = \"us-central1\" # REPLACE WITH YOUR BUCKET REGION e.g. us-central1\n",
    "\n",
    "# do not change these\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"TFVERSION\"] = \"1.13\"  # Tensorflow version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "gs = storage.Client(project=PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing image_model/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile image_model/__init__.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-gpu\n",
      "  Downloading https://files.pythonhosted.org/packages/ce/e4/be6d7d17d727b0d2e10d30a87de4e56b8661075eccb8032a8aa5cc84c300/tensorflow_gpu-1.13.1-cp35-cp35m-manylinux1_x86_64.whl (345.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 345.2MB 3.5kB/s \n",
      "\u001b[?25hCollecting keras-applications>=1.0.6 (from tensorflow-gpu)\n",
      "  Using cached https://files.pythonhosted.org/packages/90/85/64c82949765cfb246bbdaf5aca2d55f400f792655927a017710a78445def/Keras_Applications-1.0.7-py2.py3-none-any.whl\n",
      "Collecting six>=1.10.0 (from tensorflow-gpu)\n",
      "  Using cached https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
      "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0 (from tensorflow-gpu)\n",
      "  Using cached https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl\n",
      "Collecting protobuf>=3.6.1 (from tensorflow-gpu)\n",
      "  Using cached https://files.pythonhosted.org/packages/81/59/c7b0815a78fd641141f24a6ece878293eae6bf1fce40632a6ab9672346aa/protobuf-3.7.1-cp35-cp35m-manylinux1_x86_64.whl\n",
      "Collecting astor>=0.6.0 (from tensorflow-gpu)\n",
      "  Using cached https://files.pythonhosted.org/packages/35/6b/11530768cac581a12952a2aad00e1526b89d242d0b9f59534ef6e6a1752f/astor-0.7.1-py2.py3-none-any.whl\n",
      "Collecting keras-preprocessing>=1.0.5 (from tensorflow-gpu)\n",
      "  Using cached https://files.pythonhosted.org/packages/c0/bf/0315ef6a9fd3fc2346e85b0ff1f5f83ca17073f2c31ac719ab2e4da0d4a3/Keras_Preprocessing-1.0.9-py2.py3-none-any.whl\n",
      "Collecting grpcio>=1.8.6 (from tensorflow-gpu)\n",
      "  Using cached https://files.pythonhosted.org/packages/0e/fd/e6696e5b115f328c382dd88414168e2b918cb7153b59dc9228d3c15e356c/grpcio-1.19.0-cp35-cp35m-manylinux1_x86_64.whl\n",
      "Collecting tensorboard<1.14.0,>=1.13.0 (from tensorflow-gpu)\n",
      "  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.2MB 558kB/s \n",
      "\u001b[?25hCollecting absl-py>=0.1.6 (from tensorflow-gpu)\n",
      "Collecting wheel>=0.26 (from tensorflow-gpu)\n",
      "  Using cached https://files.pythonhosted.org/packages/96/ba/a4702cbb6a3a485239fbe9525443446203f00771af9ac000fa3ef2788201/wheel-0.33.1-py2.py3-none-any.whl\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-gpu)\n",
      "Collecting numpy>=1.13.3 (from tensorflow-gpu)\n",
      "  Using cached https://files.pythonhosted.org/packages/e3/18/4f013c3c3051f4e0ffbaa4bf247050d6d5e527fe9cb1907f5975b172f23f/numpy-1.16.2-cp35-cp35m-manylinux1_x86_64.whl\n",
      "Collecting gast>=0.2.0 (from tensorflow-gpu)\n",
      "Collecting h5py (from keras-applications>=1.0.6->tensorflow-gpu)\n",
      "  Using cached https://files.pythonhosted.org/packages/4c/77/c4933e12dca0f61bcdafc207c7532e1250b8d12719459fd85132f3daa9fd/h5py-2.9.0-cp35-cp35m-manylinux1_x86_64.whl\n",
      "Collecting mock>=2.0.0 (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow-gpu)\n",
      "  Using cached https://files.pythonhosted.org/packages/e6/35/f187bdf23be87092bd0f1200d43d23076cee4d0dec109f195173fd3ebc79/mock-2.0.0-py2.py3-none-any.whl\n",
      "Collecting setuptools (from protobuf>=3.6.1->tensorflow-gpu)\n",
      "  Downloading https://files.pythonhosted.org/packages/44/56/75e64a8fbbe9e0bd30cfdd58ca1856bc0dc15a43e41504a58d8373f34213/setuptools-40.9.0-py2.py3-none-any.whl (575kB)\n",
      "\u001b[K    100% |████████████████████████████████| 583kB 2.6MB/s \n",
      "\u001b[?25hCollecting markdown>=2.6.8 (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu)\n",
      "  Using cached https://files.pythonhosted.org/packages/f5/e4/d8c18f2555add57ff21bf25af36d827145896a07607486cc79a2aea641af/Markdown-3.1-py2.py3-none-any.whl\n",
      "Collecting werkzeug>=0.11.15 (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu)\n",
      "  Using cached https://files.pythonhosted.org/packages/18/79/84f02539cc181cdbf5ff5a41b9f52cae870b6f632767e43ba6ac70132e92/Werkzeug-0.15.2-py2.py3-none-any.whl\n",
      "Collecting pbr>=0.11 (from mock>=2.0.0->tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow-gpu)\n",
      "  Using cached https://files.pythonhosted.org/packages/14/09/12fe9a14237a6b7e0ba3a8d6fcf254bf4b10ec56a0185f73d651145e9222/pbr-5.1.3-py2.py3-none-any.whl\n",
      "Installing collected packages: numpy, six, h5py, keras-applications, absl-py, pbr, mock, tensorflow-estimator, setuptools, protobuf, astor, keras-preprocessing, grpcio, markdown, wheel, werkzeug, tensorboard, termcolor, gast, tensorflow-gpu\n",
      "Successfully installed absl-py-0.7.1 astor-0.7.1 gast-0.2.2 grpcio-1.19.0 h5py-2.9.0 keras-applications-1.0.7 keras-preprocessing-1.0.9 markdown-3.1 mock-2.0.0 numpy-1.16.2 pbr-5.1.3 protobuf-3.7.1 setuptools-40.9.0 six-1.12.0 tensorboard-1.13.1 tensorflow-estimator-1.13.0 tensorflow-gpu-1.13.1 termcolor-1.1.0 werkzeug-0.15.2 wheel-0.33.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 5959332707925575775\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 13996827382556310288\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_GPU:1\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 13972644753453134202\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_GPU:2\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 16718463893293724108\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_GPU:3\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 5395195084528230550\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_GPU:4\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 40863013604845079\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_GPU:5\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 8085925782365590015\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_GPU:6\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 4890043033057284767\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_GPU:7\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 1226875941274768528\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 12307343873855142079\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 15651900621\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "    link {\n",
      "      device_id: 1\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "    link {\n",
      "      device_id: 2\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "    link {\n",
      "      device_id: 3\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "    link {\n",
      "      device_id: 5\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "incarnation: 14820377939934320445\n",
      "physical_device_desc: \"device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\"\n",
      ", name: \"/device:GPU:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 15651900621\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "    link {\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "    link {\n",
      "      device_id: 2\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "    link {\n",
      "      device_id: 3\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "incarnation: 2148428132750164709\n",
      "physical_device_desc: \"device: 1, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:05.0, compute capability: 7.0\"\n",
      ", name: \"/device:GPU:2\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 15651900621\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "    link {\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "    link {\n",
      "      device_id: 1\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "    link {\n",
      "      device_id: 3\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "    link {\n",
      "      device_id: 7\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "incarnation: 6177512271150075939\n",
      "physical_device_desc: \"device: 2, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:06.0, compute capability: 7.0\"\n",
      ", name: \"/device:GPU:3\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 15651900621\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "    link {\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "    link {\n",
      "      device_id: 1\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "    link {\n",
      "      device_id: 2\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "incarnation: 7490195266780872652\n",
      "physical_device_desc: \"device: 3, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:07.0, compute capability: 7.0\"\n",
      ", name: \"/device:GPU:4\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 15651900621\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "    link {\n",
      "      device_id: 5\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "    link {\n",
      "      device_id: 6\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "    link {\n",
      "      device_id: 7\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "incarnation: 12238510637665513104\n",
      "physical_device_desc: \"device: 4, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:08.0, compute capability: 7.0\"\n",
      ", name: \"/device:GPU:5\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 15651900621\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "    link {\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "    link {\n",
      "      device_id: 4\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "    link {\n",
      "      device_id: 6\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "    link {\n",
      "      device_id: 7\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "incarnation: 14735249525249096922\n",
      "physical_device_desc: \"device: 5, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:09.0, compute capability: 7.0\"\n",
      ", name: \"/device:GPU:6\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 15651900621\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "    link {\n",
      "      device_id: 4\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "    link {\n",
      "      device_id: 5\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "    link {\n",
      "      device_id: 7\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "incarnation: 8566792415329654853\n",
      "physical_device_desc: \"device: 6, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:0a.0, compute capability: 7.0\"\n",
      ", name: \"/device:GPU:7\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 15651900621\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "    link {\n",
      "      device_id: 2\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "    link {\n",
      "      device_id: 4\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "    link {\n",
      "      device_id: 5\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "    link {\n",
      "      device_id: 6\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "incarnation: 11976821837725058147\n",
      "physical_device_desc: \"device: 7, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:0b.0, compute capability: 7.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting image_model/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile image_model/task.py\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "from . import model\n",
    "import tensorflow as tf\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser()\n",
    "  # Input Arguments\n",
    "  parser.add_argument(\n",
    "      '--batch_size',\n",
    "      help='Batch size for training steps',\n",
    "      type=int,\n",
    "      default=100\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--learning_rate',\n",
    "      help='Initial learning rate for training',\n",
    "      type=float,\n",
    "      default=0.01\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--train_steps',\n",
    "      help=\"\"\"\\\n",
    "      Steps to run the training job for. A step is one batch-size,\\\n",
    "      \"\"\",\n",
    "      type=int,\n",
    "      default=100\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--output_dir',\n",
    "      help='GCS location to write checkpoints and export models',\n",
    "      required=True\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--train_data_path',\n",
    "      help='location of train file containing eval URLs',\n",
    "      default='gs://cloud-ml-data/img/flower_photos/train_set.csv'\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--eval_data_path',\n",
    "      help='location of eval file containing img URLs',\n",
    "      default='gs://cloud-ml-data/img/flower_photos/eval_set.csv'\n",
    "  )\n",
    "  #build list of model fn's for help message\n",
    "  model_names = [name.replace('_model','') \\\n",
    "                   for name in dir(model) \\\n",
    "                     if name.endswith('_model')]  \n",
    "  parser.add_argument(\n",
    "      '--job-dir',\n",
    "      help='this model ignores this field, but it is required by gcloud',\n",
    "      default='junk'\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--augment', \n",
    "      help='if specified, augment image data', \n",
    "      dest='augment', action='store_true')\n",
    "  parser.set_defaults(augment=False)\n",
    "  parser.add_argument(\n",
    "      '--pretrained', \n",
    "      help='if specified, uses pretrained model', \n",
    "      dest='pretrained')\n",
    "  \n",
    "\n",
    "  # optional hyperparameters used by cnn\n",
    "  parser.add_argument(\n",
    "      '--ksize1', \n",
    "      help='kernel size of first layer for CNN', \n",
    "      type=int, \n",
    "      default=5)\n",
    "  parser.add_argument(\n",
    "        '--embedding_path',\n",
    "        help='OPTIONAL: can be a local path or a GCS url (gs://...). \\\n",
    "              Download from: https://nlp.stanford.edu/projects/glove/',\n",
    "    )\n",
    "  parser.add_argument(\n",
    "      '--ksize2', \n",
    "      help='kernel size of second layer for CNN', \n",
    "      type=int, \n",
    "      default=5)\n",
    "  parser.add_argument(\n",
    "      '--nfil1', \n",
    "      help='number of filters in first layer for CNN', \n",
    "      type=int, \n",
    "      default=10)\n",
    "  parser.add_argument(\n",
    "      '--nfil2', \n",
    "      help='number of filters in second layer for CNN', \n",
    "      type=int, \n",
    "      default=20)\n",
    "  parser.add_argument(\n",
    "        '--num_epochs',\n",
    "        help='number of times to go through the data, default=10',\n",
    "        default=10,\n",
    "        type=float\n",
    "    )\n",
    "  parser.add_argument(\n",
    "      '--dprob', \n",
    "      help='dropout probability for CNN', \n",
    "      type=float, \n",
    "      default=0.25)\n",
    "    \n",
    "  parser.add_argument(\n",
    "      '--batch_norm', \n",
    "      help='if specified, do batch_norm for CNN', \n",
    "      dest='batch_norm', \n",
    "      action='store_true')\n",
    "  parser.set_defaults(batch_norm=False)\n",
    "\n",
    "  args = parser.parse_args()\n",
    "  hparams = args.__dict__\n",
    "  print(hparams)\n",
    "    \n",
    "  output_dir = hparams.pop('output_dir')\n",
    "  # Append trial_id to path for hptuning\n",
    "  output_dir = os.path.join(\n",
    "      output_dir,\n",
    "      json.loads(\n",
    "          os.environ.get('TF_CONFIG', '{}')\n",
    "      ).get('task', {}).get('trial', '')\n",
    "  )  \n",
    "#   print(hparams)\n",
    "  # Run the training job\n",
    "  model.train_image(output_dir, hparams, model=hparams['pretrained'])\n",
    "#   model.train_description(output_dir, hparams)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.yml\n",
    "trainingInput:\n",
    "  scaleTier: CUSTOM\n",
    "  masterType: complex_model_l_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting image_model/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile image_model/model.py\n",
    "from tensorflow.python.keras.preprocessing import text\n",
    "from tensorflow.python.keras import models\n",
    "#from tensorflow.python.keras.layers import Dense\n",
    "#from tensorflow.python.keras.layers import Dropout\n",
    "from tensorflow.python.keras.layers import Embedding\n",
    "from tensorflow.python.keras.layers import Conv1D\n",
    "from tensorflow.python.keras.layers import MaxPooling1D\n",
    "from tensorflow.python.keras.layers import GlobalAveragePooling1D\n",
    "\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input as preprocess_resnet50\n",
    "from tensorflow.keras.applications.resnet50 import  decode_predictions\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input as preprocess_vgg16\n",
    "\n",
    "from tensorflow.keras.layers import Reshape, Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import average \n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "CLASSES_LIST = ['Bras', 'Ties', 'Tops', 'Jeans', 'Polos', 'Rings', 'Socks',\n",
    "       'Skirts', 'Watches', 'Leggings', 'Sweaters', 'T-Shirts',\n",
    "       'Necklaces', 'Swim Tops', 'Underwear', 'Fragrances', 'Range Hoods',\n",
    "       'Basins/Sinks', 'Button-Downs', 'Slacks/Pants', 'Swim Bottoms',\n",
    "       'Jackets/Coats', 'Office Chairs', 'Gloves/Mittens',\n",
    "       'Semi-Brim Hats', 'Dresses & Gowns', 'Pendants & Charms',\n",
    "       'Blazers/Suit Coats', 'Swim Variety Packs', 'Bracelets & Anklets',\n",
    "       'One-Piece Swimsuits', 'Protective Footwear',\n",
    "       'Faucets/Taps/Handles', 'Bedding Variety Packs',\n",
    "       'Earrings & Ear Jewelry', 'Protective/Active Tops',\n",
    "       'Cardigans/Kimonos/Wraps', 'Everyday/Dress Footwear',\n",
    "       'Protective/Active Pants', 'Protective/Active Vests',\n",
    "       'Tableware Variety Packs', 'Active/Athletic Footwear',\n",
    "       'Protective/Active Shorts', 'Specialty Sport Footwear',\n",
    "       'Hair Cleaning & Treatments', 'Business/Formal Dress Suits',\n",
    "       'Sweatshirts/Fleece Pullovers', 'Clothing Sets & Variety Packs',\n",
    "       'Protective/Active Button-Downs',\n",
    "       'Vitamins, Minerals, & Dietary Supplements']\n",
    "CLASSES = { CLASSES_LIST[i]: i for i in range(len(CLASSES_LIST))}\n",
    "\n",
    "# For image classification\n",
    "HEIGHT = 224\n",
    "WIDTH = 224\n",
    "NUM_CHANNELS = 3\n",
    "NCLASSES = 50\n",
    "\n",
    "# For text classification\n",
    "TOP_K = 20000  # Limit on the number vocabulary size used for tokenization\n",
    "MAX_SEQUENCE_LENGTH = 500  # Sentences will be truncated/padded to this length\n",
    "PADWORD = 'ZYXW'\n",
    "\n",
    "\n",
    "\n",
    "# For image classification\n",
    "def read_and_preprocess_with_augment(image_bytes, bucket_name=None, product_name=None, description=None, bucket_label=None, pretrained='none'):\n",
    "    return read_and_preprocess(image_bytes=image_bytes, bucket_name=bucket_name, product_name=product_name, description=description, bucket_label=bucket_label, augment=True, pretrained=pretrained)\n",
    "\n",
    "\n",
    "# For image classification\n",
    "def read_and_preprocess(image_bytes, bucket_name=None, product_name=None, description=None, bucket_label=None, augment=False, pretrained='none'):\n",
    "    # Decode the image, end up with pixel values that are in the -1, 1 range\n",
    "    image = tf.image.decode_jpeg(contents = image_bytes, channels=NUM_CHANNELS)\n",
    "    image = tf.expand_dims(input = image, axis = 0) # resize_bilinear needs batches\n",
    "\n",
    "    if augment:\n",
    "        image = tf.image.resize_bilinear(images = image, size = [HEIGHT+10, WIDTH+10], align_corners = False)\n",
    "        image = tf.squeeze(input = image, axis = 0) # remove batch dimension\n",
    "        image = tf.random_crop(value = image, size = [HEIGHT, WIDTH, NUM_CHANNELS])\n",
    "        image = tf.image.random_flip_left_right(image = image)\n",
    "        image = tf.image.random_brightness(image = image, max_delta = 63.0/255.0)\n",
    "        image = tf.image.random_contrast(image = image, lower = 0.2, upper = 1.8)\n",
    "    else:\n",
    "        image = tf.image.resize_bilinear(images = image, size = [HEIGHT, WIDTH], align_corners = False)\n",
    "        image = tf.squeeze(input = image, axis = 0) #remove batch dimension\n",
    "\n",
    "    # Pixel values are in range [0,1], convert to [-1,1]\n",
    "    if pretrained == 'none':\n",
    "        image = tf.image.convert_image_dtype(image = image, dtype = tf.float32) # 0-1\n",
    "        image = tf.subtract(x = image, y = 0.5)\n",
    "        image = tf.multiply(x = image, y = 2.0)\n",
    "    elif pretrained == 'res_50':\n",
    "        image = preprocess_resnet50(image)\n",
    "    elif pretrained == 'vgg16':\n",
    "        image = preprocess_vgg16(image)    \n",
    "    else:\n",
    "        raise Exception('unknown pretrained model {}'.format(pretrained))\n",
    "\n",
    "    return image, tf.one_hot(bucket_label, 50)\n",
    "    return {'imagem':image}, bucket_name\n",
    "\n",
    "\n",
    "# For image classification\n",
    "def load_data(train_data_path, eval_data_path):\n",
    "    column_names = ('bucket_name', 'product_id', 'product_name', 'description')\n",
    "\n",
    "    def download_from_gcs(source, destination):\n",
    "        search = re.search('gs://(.*?)/(.*)', source)\n",
    "        bucket_name = search.group(1)\n",
    "        blob_name = search.group(2)\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.get_bucket(bucket_name)\n",
    "        bucket.blob(blob_name).download_to_filename(destination)\n",
    "\n",
    "    if train_data_path.startswith('gs://'):\n",
    "        download_from_gcs(train_data_path, destination='train.csv')\n",
    "        train_data_path = 'train.csv'\n",
    "    if eval_data_path.startswith('gs://'):\n",
    "        download_from_gcs(eval_data_path, destination='eval.csv')\n",
    "        eval_data_path = 'eval.csv'\n",
    "\n",
    "    def download_image(product_id, bucket_name, product_name, description, bucket_label):\n",
    "        image_bytes = tf.read_file(filename = product_id)\n",
    "        return image_bytes, bucket_name, product_name, description, bucket_label\n",
    "    \n",
    "    # Parse CSV using pandas\n",
    "    df_train = pd.read_csv(train_data_path)\n",
    "    df_eval = pd.read_csv(eval_data_path)\n",
    "    \n",
    "    df_train['bucket_label'] = df_train['bucket_name'].map(CLASSES)\n",
    "    df_eval['bucket_label'] = df_eval['bucket_name'].map(CLASSES)\n",
    "    \n",
    "    tf_train = tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(df_train['product_id'].values, tf.string),\n",
    "            tf.cast(df_train['bucket_name'].values, tf.string),\n",
    "            tf.cast(df_train['product_name'].values, tf.string),\n",
    "            tf.cast(df_train['description'].values, tf.string),\n",
    "            tf.cast(df_train['bucket_label'].values, tf.int32)\n",
    "        )\n",
    "    ).map(download_image)\n",
    "    \n",
    "    tf_eval = tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(df_train['product_id'].values, tf.string),\n",
    "            tf.cast(df_train['bucket_name'].values, tf.string),\n",
    "            tf.cast(df_train['product_name'].values, tf.string),\n",
    "            tf.cast(df_train['description'].values, tf.string),\n",
    "            tf.cast(df_train['bucket_label'].values, tf.int32)\n",
    "        )\n",
    "    ).map(download_image)\n",
    "\n",
    "    return (\n",
    "        (\n",
    "            list(df_train['product_name']),\n",
    "            list(df_train['description']),\n",
    "            list(df_train['product_id']),\n",
    "            np.array(df_train['bucket_name'].map(CLASSES)),\n",
    "            tf_train\n",
    "        ),\n",
    "        (\n",
    "            list(df_eval['product_name']),\n",
    "            list(df_eval['description']),\n",
    "            list(df_eval['product_id']),\n",
    "            np.array(df_eval['bucket_name'].map(CLASSES)),\n",
    "            tf_eval\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# This will read the dataset for both image and text classification\n",
    "def make_image_input_fn(dataset, batch_size, mode, augment = False, pretrained = False):\n",
    "    if augment: \n",
    "        dataset = dataset.map(map_func = read_and_preprocess_with_augment)\n",
    "    else:\n",
    "        dataset = dataset.map(map_func = read_and_preprocess)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        num_epochs = None\n",
    "        print('num epochs:', num_epochs)\n",
    "        # indefinitely\n",
    "        dataset = dataset.shuffle(buffer_size = 5 * batch_size)\n",
    "    else:\n",
    "        num_epochs = None # end-of-input after this\n",
    "\n",
    "    dataset = dataset.repeat(count = num_epochs).batch(batch_size = batch_size)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "For text classificatoin\n",
    "Create tf.estimator compatible input function\n",
    "  # Arguments:\n",
    "      texts: [strings], list of sentences\n",
    "      labels: numpy int vector, integer labels for sentences\n",
    "      batch_size: int, number of records to use for each train batch\n",
    "      mode: tf.estimator.ModeKeys.TRAIN or tf.estimator.ModeKeys.EVAL \n",
    "  # Returns:\n",
    "      tf.data.Dataset, produces feature and label\n",
    "        tensors one batch at a time\n",
    "\"\"\"\n",
    "def input_fn(texts, labels, batch_size, mode):\n",
    "    # Convert texts from python strings to tensors\n",
    "    x = tf.constant(texts)\n",
    "\n",
    "    # Map text to sequence of word-integers and pad\n",
    "    x = vectorize_sentences(x)\n",
    "\n",
    "    # Create tf.data.Dataset from tensors\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, labels))\n",
    "\n",
    "    # Pad to constant length\n",
    "    dataset = dataset.map(pad)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        num_epochs = None #loop indefinitley\n",
    "        dataset = dataset.shuffle(buffer_size=50000) # our input is already shuffled so this is redundant\n",
    "    else:\n",
    "        num_epochs = 1\n",
    "\n",
    "    dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "For text classification\n",
    "Given an int tensor, remove 0s then pad to a fixed length representation. \n",
    "  #Arguments:\n",
    "    feature: int tensor \n",
    "    label: int. not used in function, just passed through\n",
    "  #Returns:\n",
    "    (int tensor, int) tuple.\n",
    "\"\"\"\n",
    "def pad(feature, label):\n",
    "    # 1. Remove 0s which represent out of vocabulary words\n",
    "    nonzero_indices = tf.where(tf.not_equal(feature, tf.zeros_like(feature)))\n",
    "    without_zeros = tf.gather(feature,nonzero_indices)\n",
    "    without_zeros = tf.squeeze(without_zeros, axis=1)\n",
    "\n",
    "    # 2. Prepend 0s till MAX_SEQUENCE_LENGTH\n",
    "    padded = tf.pad(without_zeros, [[MAX_SEQUENCE_LENGTH, 0]])  # pad out with zeros\n",
    "    padded = padded[-MAX_SEQUENCE_LENGTH:]  # slice to constant length\n",
    "    return (padded, label)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "For text classification\n",
    "Given sentences, return an integer representation\n",
    "  # Arguments:\n",
    "      sentences: string tensor of shape (?,), contains sentences to vectorize\n",
    "  # Returns:\n",
    "      Integer representation of the sentence. Word-integer mapping is determined\n",
    "        by VOCAB_FILE_PATH. Words out of vocabulary will map to 0\n",
    "\"\"\"\n",
    "def vectorize_sentences(sentences):\n",
    "    # 1. Remove punctuation\n",
    "    sentences = tf.regex_replace(sentences, '[[:punct:]]', ' ')\n",
    "\n",
    "    # 2. Split string tensor into component words\n",
    "    words = tf.string_split(sentences)\n",
    "    words = tf.sparse_tensor_to_dense(words, default_value=PADWORD)\n",
    "\n",
    "    # 3. Map each word to respective integer\n",
    "    table = tf.contrib.lookup.index_table_from_file(\n",
    "        vocabulary_file=VOCAB_FILE_PATH,\n",
    "        num_oov_buckets=0,\n",
    "        vocab_size=None,\n",
    "        default_value=0,  # for words not in vocabulary (OOV)\n",
    "        key_column_index=0,\n",
    "        value_column_index=1,\n",
    "        delimiter=',')\n",
    "    numbers = table.lookup(words)\n",
    "\n",
    "    return numbers\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "For text classification\n",
    "Builds a CNN model using keras and converts to tf.estimator.Estimator\n",
    "  # Arguments\n",
    "      model_dir: string, file path where training files will be written\n",
    "      config: tf.estimator.RunConfig, specifies properties of tf Estimator\n",
    "      filters: int, output dimension of the layers.\n",
    "      kernel_size: int, length of the convolution window.\n",
    "      embedding_dim: int, dimension of the embedding vectors.\n",
    "      dropout_rate: float, percentage of input to drop at Dropout layers.\n",
    "      pool_size: int, factor by which to downscale input at MaxPooling layer.\n",
    "      embedding_path: string , file location of pre-trained embedding (if used)\n",
    "        defaults to None which will cause the model to train embedding from scratch\n",
    "      word_index: dictionary, mapping of vocabulary to integers. used only if\n",
    "        pre-trained embedding is provided\n",
    "\n",
    "    # Returns\n",
    "        A tf.estimator.Estimator \n",
    "\"\"\"\n",
    "def keras_estimator(model_dir,\n",
    "                    config,\n",
    "                    learning_rate,\n",
    "                    filters=64,\n",
    "                    dropout_rate=0.2,\n",
    "                    embedding_dim=200,\n",
    "                    kernel_size=3,\n",
    "                    pool_size=3,\n",
    "                    embedding_path=None,\n",
    "                    word_index=None):\n",
    "    # Create model instance.\n",
    "#     model = models.Sequential()\n",
    "    num_features = min(len(word_index) + 1, TOP_K)\n",
    "\n",
    "    # Add embedding layer. If pre-trained embedding is used add weights to the\n",
    "    # embeddings layer and set trainable to input is_embedding_trainable flag.\n",
    "    input_tensor = Input(shape=(None,),\n",
    "                        dtype='int32', \n",
    "                        name='input_text')\n",
    "    if embedding_path != None:\n",
    "        embedding_matrix = get_embedding_matrix(word_index, embedding_path, embedding_dim)\n",
    "        is_embedding_trainable = True  # set to False to freeze embedding weights\n",
    "\n",
    "        text_input = Embedding(input_dim=num_features,\n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=is_embedding_trainable)(input_tensor)\n",
    "    else:\n",
    "        text_input = Embedding(input_dim=num_features,\n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH)(input_tensor)\n",
    "\n",
    "#     text_input = Flatten()(text_input)\n",
    "    x = Dropout(rate=dropout_rate)(text_input)\n",
    "    x = Conv1D(filters=filters,\n",
    "                              kernel_size=kernel_size,\n",
    "                              activation='relu',\n",
    "                              bias_initializer='random_uniform',\n",
    "                              padding='same')(x)\n",
    "\n",
    "    x = MaxPooling1D(pool_size=pool_size)(x)\n",
    "    x = Conv1D(filters=filters * 2,\n",
    "                              kernel_size=kernel_size,\n",
    "                              activation='relu',\n",
    "                              bias_initializer='random_uniform',\n",
    "                              padding='same')(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(rate=dropout_rate)(x)\n",
    "    predictions = Dense(len(CLASSES), activation='softmax')(x)\n",
    "    \n",
    "    #specify the model\n",
    "    model_text = tf.keras.Model(inputs=input_tensor, outputs = predictions)\n",
    "\n",
    "    # Compile model with learning parameters.\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model_text.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "    model_text.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "    estimator = tf.keras.estimator.model_to_estimator(keras_model=model_text, model_dir=model_dir, config=config)\n",
    "\n",
    "    \n",
    "    return estimator\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "For text classification\n",
    "Defines the features to be passed to the model during inference\n",
    "  Can pass in string text directly. Tokenization done in serving_input_fn \n",
    "  # Arguments: none\n",
    "  # Returns: tf.estimator.export.ServingInputReceiver\n",
    "\"\"\"\n",
    "def serving_input_fn():\n",
    "    feature_placeholder = tf.placeholder(tf.string, [None])\n",
    "    features = vectorize_sentences(feature_placeholder)\n",
    "    return tf.estimator.export.TensorServingInputReceiver(features, feature_placeholder)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "For text classification\n",
    "Takes embedding for generic vocabulary and extracts the embeddings\n",
    "  matching the current vocabulary\n",
    "  The pre-trained embedding file is obtained from https://nlp.stanford.edu/projects/glove/\n",
    "  # Arguments: \n",
    "      word_index: dict, {key =word in vocabulary: value= integer mapped to that word}\n",
    "      embedding_path: string, location of the pre-trained embedding file on disk\n",
    "      embedding_dim: int, dimension of the embedding space\n",
    "  # Returns: numpy matrix of shape (vocabulary, embedding_dim) that contains the embedded\n",
    "      representation of each word in the vocabulary.\n",
    "\"\"\"\n",
    "def get_embedding_matrix(word_index, embedding_path, embedding_dim):\n",
    "    # Read the pre-trained embedding file and get word to word vector mappings.\n",
    "    embedding_matrix_all = {}\n",
    "\n",
    "    # Download if embedding file is in GCS\n",
    "    if embedding_path.startswith('gs://'):\n",
    "        download_from_gcs(embedding_path, destination='embedding.csv')\n",
    "        embedding_path = 'embedding.csv'\n",
    "\n",
    "    with open(embedding_path) as f:\n",
    "        for line in f:  # Every line contains word followed by the vector value\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embedding_matrix_all[word] = coefs\n",
    "\n",
    "    # Prepare embedding matrix with just the words in our word_index dictionary\n",
    "    num_words = min(len(word_index) + 1, TOP_K)\n",
    "    embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        if i >= TOP_K:\n",
    "            continue\n",
    "        embedding_vector = embedding_matrix_all.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Main orchestrator for text classification\n",
    "\"\"\"\n",
    "def _train_text(output_dir, hparams, field):\n",
    "    # Load Data\n",
    "    (\n",
    "        (train_product_name, train_description, _, train_labels, _),\n",
    "        (test_product_name, test_description, _, test_labels, _)\n",
    "    ) = load_data(hparams['train_data_path'], hparams['eval_data_path'])\n",
    "    \n",
    "    if field == 'product_name':\n",
    "        train_texts = train_product_name\n",
    "        test_texts = test_product_name\n",
    "    else:\n",
    "        train_texts = train_description\n",
    "        test_texts = test_description\n",
    "\n",
    "    # Create vocabulary from training corpus.\n",
    "    tokenizer = text.Tokenizer(num_words=TOP_K)\n",
    "    tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "    # Generate vocabulary file from tokenizer object to enable\n",
    "    # creating a native tensorflow lookup table later (used in vectorize_sentences())\n",
    "    tf.gfile.MkDir(output_dir) # directory must exist before we can use tf.gfile.open\n",
    "    global VOCAB_FILE_PATH; VOCAB_FILE_PATH = os.path.join(output_dir,'vocab.txt')\n",
    "    with tf.gfile.Open(VOCAB_FILE_PATH, 'wb') as f:\n",
    "        f.write(\"{},0\\n\".format(PADWORD))  # map padword to 0\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index < TOP_K: # only save mappings for TOP_K words\n",
    "                f.write(\"{},{}\\n\".format(word, index))\n",
    "\n",
    "    # Create estimator\n",
    "    run_config = tf.estimator.RunConfig(save_checkpoints_steps=500)\n",
    "    estimator = keras_estimator(\n",
    "        model_dir=output_dir,\n",
    "        config=run_config,\n",
    "        learning_rate=hparams['learning_rate'],\n",
    "        embedding_path=hparams['embedding_path'],\n",
    "        word_index=tokenizer.word_index\n",
    "    )\n",
    "\n",
    "    # Create TrainSpec\n",
    "    train_steps = hparams['num_epochs'] * len(train_texts) / hparams['batch_size']\n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn=lambda:input_fn(\n",
    "            train_texts,\n",
    "            train_labels,\n",
    "            hparams['batch_size'],\n",
    "            mode=tf.estimator.ModeKeys.TRAIN),\n",
    "        max_steps=train_steps\n",
    "    )\n",
    "\n",
    "    # Create EvalSpec\n",
    "    exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn=lambda:input_fn(\n",
    "            test_texts,\n",
    "            test_labels,\n",
    "            hparams['batch_size'],\n",
    "            mode=tf.estimator.ModeKeys.EVAL),\n",
    "        steps=None,\n",
    "        exporters=exporter,\n",
    "        start_delay_secs=10,\n",
    "        throttle_secs=10\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Main orchestrator for product_name\n",
    "\"\"\"\n",
    "def train_product_name(output_dir, hparams):\n",
    "    return _train_text(output_dir, hparams, 'product_name')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Main orchestrator for product_name\n",
    "\"\"\"\n",
    "def train_description(output_dir, hparams):\n",
    "    return _train_text(output_dir, hparams, 'description')\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Main orchestrator for image classification\n",
    "\"\"\"\n",
    "def train_image(output_dir, hparams, model = 'res_50'):\n",
    "    (\n",
    "            (train_product_name, train_description, train_imgurl, train_labels, train_tfset),\n",
    "            (test_product_name, test_description, test_imgurl, test_labels, eval_tfset)\n",
    "     ) = load_data(hparams['train_data_path'], hparams['eval_data_path'])\n",
    "\n",
    "\n",
    "      \n",
    "    train_input_fn = make_image_input_fn(train_tfset, 50, tf.estimator.ModeKeys.TRAIN, pretrained=hparams['pretrained'])\n",
    "    eval_input_fn = make_image_input_fn(eval_tfset, 50, tf.estimator.ModeKeys.EVAL, pretrained=hparams['pretrained'])\n",
    "    print('train_input_fn', train_input_fn)\n",
    "    print('')\n",
    "    print('')\n",
    "    print('')\n",
    "    print('')\n",
    "    print('')\n",
    "    print('')\n",
    "    if model=='res_50':\n",
    "  # connect new layers to the output\n",
    "        res_model = ResNet50(weights='imagenet', include_top=False,  input_shape=(224, 224, 3))\n",
    "        layer_dict = dict([(layer.name, layer) for layer in res_model.layers])\n",
    "        x = layer_dict['activation_48'].output\n",
    "        x = Flatten()(x)\n",
    "        # let's add a fully-connected layer\n",
    "        x = Dense(100, activation='relu',kernel_initializer='he_uniform')(x)\n",
    "        # and a fully connected layer \n",
    "        predictions = Dense(50, activation='softmax', kernel_initializer='glorot_uniform')(x)\n",
    "\n",
    "        Res50 = tf.keras.Model(inputs=res_model.input, outputs=predictions)\n",
    "\n",
    "        # freeze ResNet during training\n",
    "        for layer in res_model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        Res50.compile(optimizer='adam',\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        Res50.fit(train_input_fn,\n",
    "                  validation_data=eval_input_fn,\n",
    "                  epochs=1,\n",
    "                  validation_steps=209,\n",
    "                  steps_per_epoch=1)\n",
    "        export_path = tf.contrib.saved_model.save_keras_model(Res50, output_dir)\n",
    "        print(\"Model exported to: \", export_path)\n",
    "        return\n",
    "    elif model == 'vgg16':\n",
    "\n",
    "        #Load the VGG model\n",
    "        vgg_16 = VGG16(weights='imagenet', include_top=False,  input_shape=(224, 224, 3))\n",
    "        # Freeze the layers except the last 4 layers\n",
    "        for layer in vgg_16.layers[:-4]:\n",
    "            layer.trainable = False\n",
    "\n",
    "        # Getting output tensor of the last VGG layer that we want to include\n",
    "        layer_dict = dict([(layer.name, layer) for layer in vgg_16.layers])\n",
    "        x = layer_dict['block2_pool'].output\n",
    "        \n",
    "#         flat = Flatten()(x)\n",
    "#         x = Dropout(0.20)(flat)\n",
    "        \n",
    "        x = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
    "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "        x = Flatten()(x)        \n",
    "        \n",
    "        x = Dense(1024, activation='relu',kernel_initializer='he_uniform')(x)\n",
    "        x = Dropout(0.4)(x)\n",
    "        predictions = Dense(50, activation='softmax', kernel_initializer='glorot_uniform')(x)\n",
    "        vgg16 = tf.keras.Model(inputs=vgg_16.input, outputs = predictions)\n",
    "        vgg16.compile(\n",
    "              optimizer=optimizers.RMSprop(lr=1e-5),\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "        vgg16.fit(train_input_fn,\n",
    "                  validation_data=eval_input_fn,\n",
    "                  epochs=20,\n",
    "                  validation_steps=209,#8348/40 len(df)/batch_size\n",
    "                  steps_per_epoch=712 ) #28477/40 len(df)/batch_size)     \n",
    "    else:\n",
    "        raise Exception(\"incorrect model name\")\n",
    "    print(output_dir)\n",
    "\n",
    "    estimator = tf.keras.estimator.model_to_estimator(\n",
    "        keras_model = Res50,\n",
    "        model_dir = output_dir,\n",
    "        config = tf.estimator.RunConfig(\n",
    "              tf_random_seed = 1, # for reproducibility\n",
    "              save_checkpoints_steps = 1 # checkpoint every N steps\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    def serving_input_fn():\n",
    "        features = {\n",
    "            'product_id': tf.placeholder(dtype = tf.string, shape = [None]),\n",
    "            'bucket_name': tf.placeholder(dtype = tf.string, shape = [None]),\n",
    "            'product_name': tf.placeholder(dtype = tf.string, shape = [None]),\n",
    "            'description': tf.placeholder(dtype = tf.string, shape = [None]),\n",
    "            'bucket_label': tf.placeholder(dtype = tf.int32, shape = [None]),\n",
    "        }\n",
    "        return tf.estimator.export.ServingInputReceiver(features=features, receiver_tensors=features)\n",
    "\n",
    "    train_spec=tf.estimator.TrainSpec(input_fn = train_input_fn, max_steps = 300)\n",
    "    exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n",
    "\n",
    "    eval_spec=tf.estimator.EvalSpec(\n",
    "                  input_fn = eval_input_fn,\n",
    "                  steps = None,\n",
    "                  start_delay_secs = 10, # wait at least N seconds before first evaluation (default 120)\n",
    "                  throttle_secs = 10, # wait at least N seconds before each subsequent evaluation (default 600)\n",
    "                  exporters = exporter) # export SavedModel once at the end of training\n",
    "\n",
    "    tf.logging.set_verbosity(tf.logging.INFO) # so loss is printed during training\n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "vgg_16 = VGG16(weights='imagenet', include_top=False,  input_shape=(224, 224, 3))\n",
    "vgg_16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.5/site-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 112, 112, 64) 256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 112, 112, 64) 0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, 56, 56, 64)   4160        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (None, 56, 56, 64)   256         res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 56, 56, 64)   0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, 56, 56, 64)   36928       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizati (None, 56, 56, 64)   256         res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 56, 56, 64)   0           bn2a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)         (None, 56, 56, 256)  16640       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, 56, 56, 256)  16640       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizati (None, 56, 56, 256)  1024        res2a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalizatio (None, 56, 56, 256)  1024        res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 56, 56, 256)  0           bn2a_branch2c[0][0]              \n",
      "                                                                 bn2a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 56, 56, 256)  0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (None, 56, 56, 64)   16448       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizati (None, 56, 56, 64)   256         res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 56, 56, 64)   0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (None, 56, 56, 64)   36928       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizati (None, 56, 56, 64)   256         res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 56, 56, 64)   0           bn2b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)         (None, 56, 56, 256)  16640       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizati (None, 56, 56, 256)  1024        res2b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 56, 56, 256)  0           bn2b_branch2c[0][0]              \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 56, 56, 256)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)         (None, 56, 56, 64)   16448       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizati (None, 56, 56, 64)   256         res2c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 56, 56, 64)   0           bn2c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)         (None, 56, 56, 64)   36928       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizati (None, 56, 56, 64)   256         res2c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 56, 56, 64)   0           bn2c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)         (None, 56, 56, 256)  16640       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizati (None, 56, 56, 256)  1024        res2c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 56, 56, 256)  0           bn2c_branch2c[0][0]              \n",
      "                                                                 activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 56, 56, 256)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)         (None, 28, 28, 128)  32896       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizati (None, 28, 28, 128)  512         res3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 28, 28, 128)  0           bn3a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)         (None, 28, 28, 128)  147584      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizati (None, 28, 28, 128)  512         res3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 28, 28, 128)  0           bn3a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)         (None, 28, 28, 512)  66048       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)          (None, 28, 28, 512)  131584      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizati (None, 28, 28, 512)  2048        res3a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalizatio (None, 28, 28, 512)  2048        res3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 28, 28, 512)  0           bn3a_branch2c[0][0]              \n",
      "                                                                 bn3a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 28, 28, 512)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)         (None, 28, 28, 128)  65664       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizati (None, 28, 28, 128)  512         res3b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 28, 28, 128)  0           bn3b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)         (None, 28, 28, 128)  147584      activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizati (None, 28, 28, 128)  512         res3b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 28, 28, 128)  0           bn3b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)         (None, 28, 28, 512)  66048       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizati (None, 28, 28, 512)  2048        res3b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 28, 28, 512)  0           bn3b_branch2c[0][0]              \n",
      "                                                                 activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 28, 28, 512)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)         (None, 28, 28, 128)  65664       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizati (None, 28, 28, 128)  512         res3c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 28, 28, 128)  0           bn3c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)         (None, 28, 28, 128)  147584      activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizati (None, 28, 28, 128)  512         res3c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 28, 28, 128)  0           bn3c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)         (None, 28, 28, 512)  66048       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizati (None, 28, 28, 512)  2048        res3c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 28, 28, 512)  0           bn3c_branch2c[0][0]              \n",
      "                                                                 activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 28, 28, 512)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)         (None, 28, 28, 128)  65664       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizati (None, 28, 28, 128)  512         res3d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 28, 28, 128)  0           bn3d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)         (None, 28, 28, 128)  147584      activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizati (None, 28, 28, 128)  512         res3d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 28, 28, 128)  0           bn3d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)         (None, 28, 28, 512)  66048       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizati (None, 28, 28, 512)  2048        res3d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 28, 28, 512)  0           bn3d_branch2c[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 28, 28, 512)  0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)         (None, 14, 14, 256)  131328      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 14, 14, 256)  0           bn4a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 14, 14, 256)  0           bn4a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)          (None, 14, 14, 1024) 525312      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalizatio (None, 14, 14, 1024) 4096        res4a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 14, 14, 1024) 0           bn4a_branch2c[0][0]              \n",
      "                                                                 bn4a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 14, 14, 1024) 0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 14, 14, 256)  0           bn4b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 14, 14, 256)  0           bn4b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 14, 14, 1024) 0           bn4b_branch2c[0][0]              \n",
      "                                                                 activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 14, 14, 1024) 0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 14, 14, 256)  0           bn4c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 14, 14, 256)  0           bn4c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 14, 14, 1024) 0           bn4c_branch2c[0][0]              \n",
      "                                                                 activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 14, 14, 1024) 0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 14, 14, 256)  0           bn4d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 14, 14, 256)  0           bn4d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 14, 14, 1024) 0           bn4d_branch2c[0][0]              \n",
      "                                                                 activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 14, 14, 1024) 0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4e_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 14, 14, 256)  0           bn4e_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4e_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 14, 14, 256)  0           bn4e_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4e_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 14, 14, 1024) 0           bn4e_branch2c[0][0]              \n",
      "                                                                 activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 14, 14, 1024) 0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4f_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 14, 14, 256)  0           bn4f_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4f_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 14, 14, 256)  0           bn4f_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4f_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 14, 14, 1024) 0           bn4f_branch2c[0][0]              \n",
      "                                                                 activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 14, 14, 1024) 0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)         (None, 7, 7, 512)    524800      activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizati (None, 7, 7, 512)    2048        res5a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 7, 7, 512)    0           bn5a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)         (None, 7, 7, 512)    2359808     activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizati (None, 7, 7, 512)    2048        res5a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 7, 7, 512)    0           bn5a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)         (None, 7, 7, 2048)   1050624     activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)          (None, 7, 7, 2048)   2099200     activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2c (BatchNormalizati (None, 7, 7, 2048)   8192        res5a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormalizatio (None, 7, 7, 2048)   8192        res5a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 7, 7, 2048)   0           bn5a_branch2c[0][0]              \n",
      "                                                                 bn5a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 7, 7, 2048)   0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)         (None, 7, 7, 512)    1049088     activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizati (None, 7, 7, 512)    2048        res5b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 7, 7, 512)    0           bn5b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)         (None, 7, 7, 512)    2359808     activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizati (None, 7, 7, 512)    2048        res5b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 7, 7, 512)    0           bn5b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)         (None, 7, 7, 2048)   1050624     activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2c (BatchNormalizati (None, 7, 7, 2048)   8192        res5b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 7, 7, 2048)   0           bn5b_branch2c[0][0]              \n",
      "                                                                 activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 7, 7, 2048)   0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)         (None, 7, 7, 512)    1049088     activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNormalizati (None, 7, 7, 512)    2048        res5c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 7, 7, 512)    0           bn5c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)         (None, 7, 7, 512)    2359808     activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNormalizati (None, 7, 7, 512)    2048        res5c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 7, 7, 512)    0           bn5c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2c (Conv2D)         (None, 7, 7, 2048)   1050624     activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2c (BatchNormalizati (None, 7, 7, 2048)   8192        res5c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 7, 7, 2048)   0           bn5c_branch2c[0][0]              \n",
      "                                                                 activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 7, 7, 2048)   0           add_15[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 23,587,712\n",
      "Trainable params: 23,534,592\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "res_model = ResNet50(weights='imagenet', include_top=False,  input_shape=(224, 224, 3))\n",
    "res_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil -mq cp -r gs://project-sample/dataset1 ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/Final_Project/Image_fine_tunning/dataset1_data_train_local.csv\n"
     ]
    }
   ],
   "source": [
    "!echo ${PWD}/'dataset1_data_train_local.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process is terminated.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "OUTDIR=${PWD}/'test_local'\n",
    "rm -r $OUTDIR\n",
    "# export PYTHONPATH=${PYTHONPATH}:${PWD}/foo \n",
    "python3 -m image_model.task \\\n",
    "    --output_dir=$OUTDIR \\\n",
    "    --train_steps=1000 \\\n",
    "    --learning_rate=0.01 \\\n",
    "    --batch_size=40 \\\n",
    "    --batch_norm \\\n",
    "    --pretrained='res_50' \\\n",
    "    --train_data_path=${PWD}/'dataset1_data_train_local.csv' \\\n",
    "    --eval_data_path=${PWD}/'dataset1_data_eval_local.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129M\ttest_local/1554478134/variables\n",
      "96K\ttest_local/1554478134/assets\n",
      "130M\ttest_local/1554478134\n",
      "130M\ttest_local/\n"
     ]
    }
   ],
   "source": [
    "!du -h test_local/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment takes a couple of minutes. You can watch your deployment here: https://console.cloud.google.com/mlengine/models/{MODEL_NAME}\n",
      "Creating version (this might take a few minutes)......done.\n"
     ]
    }
   ],
   "source": [
    "# Create a version of this model (you can add --async at the end of the line to make this call non blocking)\n",
    "# Additional config flags are available: https://cloud.google.com/ml-engine/reference/rest/v1/projects.models.versions\n",
    "# You can also deploy a model that is stored locally by providing a --staging-bucket=... parameter\n",
    "!echo \"Deployment takes a couple of minutes. You can watch your deployment here: https://console.cloud.google.com/mlengine/models/{MODEL_NAME}\"\n",
    "# !gcloud ml-engine models create fashion\n",
    "!gcloud ml-engine versions create v1 \\\n",
    "    --model=fashion \\\n",
    "    --origin=./test_local/1554478134 \\\n",
    "    --staging-bucket=gs://project-sample \\\n",
    "    --project=qwiklabs-gcp-3f19cbba7aa3ae63 \\\n",
    "    --runtime-version=1.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "with open(\"image.json\", \"w\") as f:\n",
    "  for e in range(10):\n",
    "    image = np.ones((224, 224, 3))\n",
    "    # the format for ML Engine online predictions is: one JSON object per line\n",
    "    data = json.dumps({\"serving_input\": image.tolist()})  # \"serving_input\" because the ServingInput layer was named \"serving\". Keras appends \"_input\"\n",
    "    f.write(data+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = !gcloud ml-engine predict --model=fashion \\\n",
    "    --json-instances image.json \\\n",
    "    --project=qwiklabs-gcp-3f19cbba7aa3ae63\\\n",
    "    --version v1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3 -m image_model.task --output_dir=test --train_steps=1000 --learning_rate=0.01 --batch_size=40 --batch_norm --pretrained=res_50 --train_data_path=gs://project-sample/dataset1_data_train.csv --eval_data_path=gs://project-sample/dataset1_data_eval.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "OUTDIR='test'\n",
    "\n",
    "# export PYTHONPATH=${PYTHONPATH}:${PWD}/foo \n",
    "python3 -m image_model.task \\\n",
    "    --output_dir=$OUTDIR \\\n",
    "    --train_steps=1000 \\\n",
    "    --learning_rate=0.01 \\\n",
    "    --batch_size=40 \\\n",
    "    --batch_norm \\\n",
    "    --pretrained='res_50' \\\n",
    "    --train_data_path='gs://project-sample/dataset1_data_train.csv' \\\n",
    "    --eval_data_path='gs://project-sample/dataset1_data_eval.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://project-sample/image_model_190404_235750/trained us-central1 image_with_estimator_2__190404_235750\n",
      "jobId: image_with_estimator_2__190404_235750\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n",
      "Job [image_with_estimator_2__190404_235750] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs describe image_with_estimator_2__190404_235750\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs stream-logs image_with_estimator_2__190404_235750\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/image_model_$(date -u +%y%m%d_%H%M%S)/trained\n",
    "JOBNAME=image_with_estimator_2_${MODEL_TYPE}_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "    --region=$REGION \\\n",
    "    --module-name=image_model.task \\\n",
    "    --package-path=${PWD}/image_model \\\n",
    "    --job-dir=$OUTDIR \\\n",
    "    --staging-bucket=gs://$BUCKET \\\n",
    "    --scale-tier=CUSTOM \\\n",
    "    --runtime-version=$TFVERSION \\\n",
    "    --config=${PWD}/config.yml \\\n",
    "    -- \\\n",
    "    --output_dir=$OUTDIR \\\n",
    "    --train_steps=1000 \\\n",
    "    --learning_rate=0.01 \\\n",
    "    --batch_size=40 \\\n",
    "    --batch_norm \\\n",
    "    --pretrained='res_50' \\\n",
    "    --train_data_path='gs://project-sample/dataset1_data_train.csv' \\\n",
    "    --eval_data_path='gs://project-sample/dataset1_data_eval.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#         --pretrained='vgg16' \\\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
